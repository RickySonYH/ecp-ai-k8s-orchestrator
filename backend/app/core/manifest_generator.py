# [advice from AI] ECP-AI Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±ê¸°
"""
Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìë™ ìƒì„±
- í…Œë„Œì‹œ ì‚¬ì–‘ ê¸°ë°˜ YAML ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
- Jinja2 í…œí”Œë¦¿ ê¸°ë°˜ ë™ì  ìƒì„±
- ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ ZIP íŒŒì¼ ìƒì„±
"""

import os
import yaml
import zipfile
from io import BytesIO
from typing import Dict, Any, List
from pathlib import Path
from jinja2 import Template, Environment, FileSystemLoader
import structlog

from .tenant_manager import TenantSpecs

logger = structlog.get_logger(__name__)


class ManifestGenerator:
    """Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self, templates_path: str = "/app/config/k8s-templates"):
        self.templates_path = Path(templates_path)
        self.jinja_env = Environment(
            loader=FileSystemLoader(str(self.templates_path)),
            autoescape=False,
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # ê¸°ë³¸ í…œí”Œë¦¿ë“¤ (ë‚´ì¥)
        self.builtin_templates = {
            "namespace": self._get_namespace_template(),
            "deployment": self._get_deployment_template(),
            "service": self._get_service_template(),
            "hpa": self._get_hpa_template(),
            "configmap": self._get_configmap_template(),
            "networkpolicy": self._get_networkpolicy_template()
        }
        
        logger.info("ManifestGenerator ì´ˆê¸°í™” ì™„ë£Œ", templates_path=str(self.templates_path))
    
    def generate_tenant_manifests(self, tenant_specs: TenantSpecs) -> Dict[str, str]:
        """
        í…Œë„Œì‹œ ì „ì²´ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± (ì‹¤ì œ ì„œë²„ êµ¬ì„± ê¸°ë°˜)
        [advice from AI] ì‹¤ì œ í•˜ë“œì›¨ì–´ ìŠ¤í™ ê³„ì‚°ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ìˆ˜ì •
        """
        logger.info("í…Œë„Œì‹œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹œì‘", tenant_id=tenant_specs.tenant_id)
        
        manifests = {}
        service_count = 3
        
        # 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        manifests["01-namespace.yaml"] = self._generate_namespace(tenant_specs)
        
        # 2. ConfigMap
        manifests["02-configmap.yaml"] = self._generate_configmap(tenant_specs)
        
        # 3. GPU ì „ìš© ì„œë¹„ìŠ¤ë“¤ (ì‹¤ì œ ì„œë²„ êµ¬ì„± ë°˜ì˜)
        gpu_services = ["tts-server", "nlp-server", "aicm-server"]
        for gpu_service in gpu_services:
            if self._should_create_gpu_service(tenant_specs, gpu_service):
                # Deployment
                manifests[f"{service_count:02d}-{gpu_service}-deployment.yaml"] = \
                    self._generate_gpu_deployment(tenant_specs, gpu_service)
                
                # Service
                manifests[f"{service_count:02d}-{gpu_service}-service.yaml"] = \
                    self._generate_service(tenant_specs, gpu_service)
                
                # HPA (GPU ì„œë¹„ìŠ¤ëŠ” ì œí•œì  ìŠ¤ì¼€ì¼ë§)
                manifests[f"{service_count:02d}-{gpu_service}-hpa.yaml"] = \
                    self._generate_gpu_hpa(tenant_specs, gpu_service)
                
                service_count += 1
        
        # 4. CPU ì „ìš© ì„œë¹„ìŠ¤ë“¤ (ìŒì„±/í…ìŠ¤íŠ¸ ì²˜ë¦¬)
        cpu_services = ["stt-server", "ta-server", "qa-server"]
        for cpu_service in cpu_services:
            if self._should_create_cpu_service(tenant_specs, cpu_service):
                # Deployment
                manifests[f"{service_count:02d}-{cpu_service}-deployment.yaml"] = \
                    self._generate_cpu_deployment(tenant_specs, cpu_service)
                
                # Service
                manifests[f"{service_count:02d}-{cpu_service}-service.yaml"] = \
                    self._generate_service(tenant_specs, cpu_service)
                
                # HPA
                manifests[f"{service_count:02d}-{cpu_service}-hpa.yaml"] = \
                    self._generate_hpa(tenant_specs, cpu_service)
                
                service_count += 1
        
        # 5. ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë¹„ìŠ¤ë“¤ (ê¸°ì¡´ ì„œë¹„ìŠ¤ ìœ ì§€)
        app_services = ["callbot", "chatbot", "advisor"]
        for app_service in app_services:
            # Deployment
            manifests[f"{service_count:02d}-{app_service}-deployment.yaml"] = \
                self._generate_deployment(tenant_specs, app_service)
            
            # Service
            manifests[f"{service_count:02d}-{app_service}-service.yaml"] = \
                self._generate_service(tenant_specs, app_service)
            
            # HPA
            manifests[f"{service_count:02d}-{app_service}-hpa.yaml"] = \
                self._generate_hpa(tenant_specs, app_service)
            
            service_count += 1
        
        # 6. ì¸í”„ë¼ ì„œë¹„ìŠ¤ë“¤ (ì‹¤ì œ ì„œë²„ êµ¬ì„± ë°˜ì˜)
        infra_services = ["nginx", "api-gateway", "postgresql", "auth-service"]
        for infra_service in infra_services:
            # Deployment
            manifests[f"{service_count:02d}-{infra_service}-deployment.yaml"] = \
                self._generate_infra_deployment(tenant_specs, infra_service)
            
            # Service
            manifests[f"{service_count:02d}-{infra_service}-service.yaml"] = \
                self._generate_service(tenant_specs, infra_service)
            
            service_count += 1
        
        # 7. VectorDB (ì–´ë“œë°”ì´ì €ìš©)
        if tenant_specs.total_channels > 0:  # ì–´ë“œë°”ì´ì €ê°€ ìˆëŠ” ê²½ìš°
            manifests[f"{service_count:02d}-vectordb-deployment.yaml"] = \
                self._generate_infra_deployment(tenant_specs, "vectordb")
            manifests[f"{service_count:02d}-vectordb-service.yaml"] = \
                self._generate_service(tenant_specs, "vectordb")
            service_count += 1
        
        # 8. ìŠ¤í† ë¦¬ì§€ (NAS)
        manifests[f"{service_count:02d}-storage-pvc.yaml"] = \
            self._generate_storage_pvc(tenant_specs)
        service_count += 1
        
        # 9. ë„¤íŠ¸ì›Œí¬ ì •ì±…
        manifests["90-networkpolicy.yaml"] = self._generate_networkpolicy(tenant_specs)
        
        # 10. ëª¨ë‹ˆí„°ë§ ì„¤ì •
        manifests["91-monitoring.yaml"] = self._generate_monitoring(tenant_specs)
        
        logger.info(
            "í…Œë„Œì‹œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ",
            tenant_id=tenant_specs.tenant_id,
            manifest_count=len(manifests),
            gpu_services=len([s for s in gpu_services if self._should_create_gpu_service(tenant_specs, s)]),
            cpu_services=len([s for s in cpu_services if self._should_create_cpu_service(tenant_specs, s)]),
            infra_services=len(infra_services) + 1  # +1 for VectorDB
        )
        
        return manifests
    
    def create_deployment_package(self, tenant_specs: TenantSpecs) -> BytesIO:
        """
        ë°°í¬ íŒ¨í‚¤ì§€ ZIP íŒŒì¼ ìƒì„±
        """
        logger.info("ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„± ì‹œì‘", tenant_id=tenant_specs.tenant_id)
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        manifests = self.generate_tenant_manifests(tenant_specs)
        
        # ZIP íŒŒì¼ ìƒì„±
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            
            # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì¶”ê°€
            for filename, content in manifests.items():
                zip_file.writestr(f"k8s-manifests/{filename}", content)
            
            # ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
            deploy_script = self._generate_deploy_script(tenant_specs)
            zip_file.writestr("deploy.sh", deploy_script)
            
            # ì‚­ì œ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
            cleanup_script = self._generate_cleanup_script(tenant_specs)
            zip_file.writestr("cleanup.sh", cleanup_script)
            
            # README ì¶”ê°€
            readme_content = self._generate_readme(tenant_specs)
            zip_file.writestr("README.md", readme_content)
            
            # í…Œë„Œì‹œ ì‚¬ì–‘ JSON ì¶”ê°€
            import json
            try:
                specs_json = json.dumps(tenant_specs.model_dump(), indent=2, ensure_ascii=False)
                zip_file.writestr("tenant-specs.json", specs_json)
            except Exception as e:
                logger.warning("í…Œë„Œì‹œ ì‚¬ì–‘ JSON ìƒì„± ì‹¤íŒ¨", error=str(e))
                # ê¸°ë³¸ ì •ë³´ë§Œ í¬í•¨
                # gpu_typeê³¼ presetì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                gpu_type_str = tenant_specs.gpu_type.value if hasattr(tenant_specs.gpu_type, 'value') else str(tenant_specs.gpu_type)
                preset_str = tenant_specs.preset.value if hasattr(tenant_specs.preset, 'value') else str(tenant_specs.preset)
                
                basic_specs = {
                    "tenant_id": tenant_specs.tenant_id,
                    "preset": preset_str,
                    "services": ["callbot", "chatbot", "advisor", "stt", "tts", "ta", "qa"]
                }
                specs_json = json.dumps(basic_specs, indent=2, ensure_ascii=False)
                zip_file.writestr("tenant-specs.json", specs_json)
        
        zip_buffer.seek(0)
        
        logger.info("ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„± ì™„ë£Œ", tenant_id=tenant_specs.tenant_id)
        return zip_buffer
    
    def generate_tenant_manifests_with_advanced_config(self, tenant_specs: TenantSpecs, advanced_config, image_config=None) -> Dict[str, str]:
        """
        [advice from AI] ê³ ê¸‰ ì„¤ì •ì„ í¬í•¨í•œ í…Œë„Œì‹œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        """
        manifests = {}
        
        try:
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
            manifests["01-namespace.yaml"] = self._generate_namespace(tenant_specs)
            
            # ì„œë¹„ìŠ¤ë³„ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± (ê³ ê¸‰ ì„¤ì • ë° ì´ë¯¸ì§€ ì„¤ì • ì ìš©)
            services = ["callbot", "chatbot", "advisor", "stt", "tts", "ta", "qa"]
            for i, service in enumerate(services, start=2):
                # ì‹¤ì œ ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                service_image_info = self._get_service_image_info(service, image_config)
                
                # Deployment (ê³ ê¸‰ ì„¤ì •, ì´ë¯¸ì§€ ì •ë³´, ë™ì  ë¦¬ì†ŒìŠ¤ ì ìš©)
                deployment_manifest = self._generate_deployment(tenant_specs, service, advanced_config, service_image_info, None)
                manifests[f"{i:02d}-{service}-deployment.yaml"] = deployment_manifest
                
                # Service
                service_manifest = self._generate_service(tenant_specs, service)
                manifests[f"{i:02d}-{service}-service.yaml"] = service_manifest
                
                # HPA (ì˜¤í† ìŠ¤ì¼€ì¼ë§ì´ í™œì„±í™”ëœ ê²½ìš°ë§Œ)
                # HPA (ê³ ê¸‰ ì„¤ì • ì ìš©) - ì„ íƒì  í™œì„±í™”
                if advanced_config and hasattr(advanced_config, 'auto_scaling') and advanced_config.auto_scaling.enabled:
                    hpa_manifest = self._generate_hpa(tenant_specs, service, advanced_config)
                    manifests[f"{i:02d}-{service}-hpa.yaml"] = hpa_manifest
                    
                    # VPA (Vertical Pod Autoscaler) - ì‹¤í—˜ì  ê¸°ëŠ¥
                    if hasattr(advanced_config.auto_scaling, 'vpa_enabled') and getattr(advanced_config.auto_scaling, 'vpa_enabled', False):
                        vpa_manifest = self._generate_vpa(tenant_specs, service, advanced_config)
                        manifests[f"{i:02d}-{service}-vpa.yaml"] = vpa_manifest
            
            # ConfigMap
            manifests["90-configmap.yaml"] = self._generate_configmap(tenant_specs)
            
            # NetworkPolicy (ë„¤íŠ¸ì›Œí¬ ì •ì±…ì´ í™œì„±í™”ëœ ê²½ìš°)
            manifests["91-networkpolicy.yaml"] = self._generate_networkpolicy(tenant_specs)
            
            # Monitoring
            manifests["92-monitoring.yaml"] = self._generate_monitoring(tenant_specs)
            
            logger.info("ê³ ê¸‰ ì„¤ì • ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ", 
                       tenant_id=tenant_specs.tenant_id,
                       manifest_count=len(manifests))
            
            return manifests
            
        except Exception as e:
            logger.error("ê³ ê¸‰ ì„¤ì • ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨", 
                        tenant_id=tenant_specs.tenant_id, 
                        error=str(e))
            # ê¸°ë³¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¡œ í´ë°±
            return self.generate_tenant_manifests(tenant_specs)
    
    def _generate_vpa(self, tenant_specs: TenantSpecs, service_name: str, advanced_config) -> str:
        """
        [advice from AI] VPA (Vertical Pod Autoscaler) ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        ë¦¬ì†ŒìŠ¤ ì¶”ì²œ ë° ìë™ ì¡°ì •ì„ ìœ„í•œ VPA ì„¤ì •
        """
        vpa_template = """---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: {{ service_name }}-vpa
  namespace: {{ namespace }}
  labels:
    app: {{ service_name }}
    tenant: {{ tenant_id }}
    component: vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ service_name }}
  updatePolicy:
    updateMode: "{{ vpa_update_mode }}"  # Off, Auto, RecommendationOnly
  resourcePolicy:
    containerPolicies:
    - containerName: {{ service_name }}
      minAllowed:
        cpu: {{ min_cpu }}
        memory: {{ min_memory }}
      maxAllowed:
        cpu: {{ max_cpu }}
        memory: {{ max_memory }}
      controlledResources:
      - cpu
      - memory
"""
        
        template = Template(vpa_template)
        
        # VPA ì„¤ì •ê°’
        vpa_config = getattr(advanced_config.auto_scaling, 'vpa_config', {})
        update_mode = vpa_config.get('update_mode', 'RecommendationOnly')
        
        # ë¦¬ì†ŒìŠ¤ ë²”ìœ„ ì„¤ì •
        min_cpu = vpa_config.get('min_cpu', '100m')
        max_cpu = vpa_config.get('max_cpu', '8000m')
        min_memory = vpa_config.get('min_memory', '128Mi')
        max_memory = vpa_config.get('max_memory', '16Gi')
        
        return template.render(
            service_name=service_name,
            namespace=f"{tenant_specs.tenant_id}-ecp-ai",
            tenant_id=tenant_specs.tenant_id,
            vpa_update_mode=update_mode,
            min_cpu=min_cpu,
            max_cpu=max_cpu,
            min_memory=min_memory,
            max_memory=max_memory
        )
    
    def _get_service_image_info(self, service_name: str, image_config=None) -> Dict[str, str]:
        """
        [advice from AI] ì„œë¹„ìŠ¤ ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        """
        if image_config and hasattr(image_config, 'service_images'):
            # ì´ë¯¸ì§€ ì„¤ì •ì—ì„œ í•´ë‹¹ ì„œë¹„ìŠ¤ ì´ë¯¸ì§€ ì°¾ê¸°
            for service_image in image_config.service_images:
                if service_image.service_name == service_name:
                    return {
                        "image": f"{service_image.registry.url}/{service_image.repository}:{service_image.selected_tag}",
                        "pull_policy": service_image.pull_policy,
                        "registry_secret": f"{service_image.registry.name}-secret" if service_image.registry.username else None
                    }
        
        # ê¸°ë³¸ ì´ë¯¸ì§€ ì •ë³´ ë°˜í™˜
        return {
            "image": f"ecp-ai/{service_name}:latest",
            "pull_policy": "Always",
            "registry_secret": None
        }
    
    def _calculate_service_resources(self, tenant_specs: TenantSpecs, service_name: str, resource_requirements=None) -> Dict[str, Any]:
        """
        [advice from AI] ì±„ë„ ìˆ˜ ê¸°ë°˜ ë™ì  ë¦¬ì†ŒìŠ¤ ê³„ì‚°
        ì‹¤ì œ ì„œë¹„ìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ CPU/Memory ìë™ ì„¤ì •
        """
        try:
            # ì„œë¹„ìŠ¤ë³„ ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ì„¤ì •
            service_resource_mapping = {
                # GPU ì„œë¹„ìŠ¤ (TTS, NLP, AICM)
                "callbot": {"base_cpu": 1000, "base_memory": 2048, "gpu_required": True},
                "chatbot": {"base_cpu": 500, "base_memory": 1024, "gpu_required": False}, 
                "advisor": {"base_cpu": 800, "base_memory": 1536, "gpu_required": True},
                "tts": {"base_cpu": 2000, "base_memory": 4096, "gpu_required": True},
                
                # CPU ì„œë¹„ìŠ¤ (STT, TA, QA)
                "stt": {"base_cpu": 4000, "base_memory": 8192, "gpu_required": False},
                "ta": {"base_cpu": 2000, "base_memory": 4096, "gpu_required": False},
                "qa": {"base_cpu": 1000, "base_memory": 2048, "gpu_required": False},
                
                # ì¸í”„ë¼ ì„œë¹„ìŠ¤
                "database": {"base_cpu": 2000, "base_memory": 4096, "gpu_required": False},
                "redis": {"base_cpu": 500, "base_memory": 1024, "gpu_required": False},
                "nginx": {"base_cpu": 200, "base_memory": 512, "gpu_required": False}
            }
            
            base_config = service_resource_mapping.get(service_name, {
                "base_cpu": 1000, "base_memory": 2048, "gpu_required": False
            })
            
            # ì±„ë„ ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ê³„ì‚°
            total_channels = self._calculate_total_channels_from_specs(tenant_specs)
            scaling_factor = self._calculate_scaling_factor(total_channels, service_name)
            
            # ë™ì  ë¦¬ì†ŒìŠ¤ ê³„ì‚°
            calculated_cpu = int(base_config["base_cpu"] * scaling_factor)
            calculated_memory = int(base_config["base_memory"] * scaling_factor)
            
            # GPU íƒ€ì…ë³„ ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ì¡°ì •
            if base_config["gpu_required"] and hasattr(tenant_specs, 'gpu_type'):
                gpu_multiplier = self._get_gpu_resource_multiplier(tenant_specs.gpu_type)
                calculated_cpu = int(calculated_cpu * gpu_multiplier)
                calculated_memory = int(calculated_memory * gpu_multiplier)
            
            # ë¦¬ì†ŒìŠ¤ ë²”ìœ„ ì œí•œ (ìµœì†Œ/ìµœëŒ€ê°’ ì ìš©)
            min_cpu, max_cpu = 100, 16000  # 100m ~ 16 cores
            min_memory, max_memory = 128, 32768  # 128Mi ~ 32Gi
            
            final_cpu = max(min_cpu, min(calculated_cpu, max_cpu))
            final_memory = max(min_memory, min(calculated_memory, max_memory))
            
            # K8s ë¦¬ì†ŒìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            resources = {
                "requests": {
                    "cpu": f"{final_cpu}m",
                    "memory": f"{final_memory}Mi"
                },
                "limits": {
                    "cpu": f"{int(final_cpu * 1.5)}m",  # requestsì˜ 1.5ë°°
                    "memory": f"{int(final_memory * 1.2)}Mi"  # requestsì˜ 1.2ë°°
                }
            }
            
            # GPU ë¦¬ì†ŒìŠ¤ ì¶”ê°€
            if base_config["gpu_required"] and hasattr(tenant_specs, 'gpu_count') and tenant_specs.gpu_count > 0:
                gpu_count = max(1, tenant_specs.gpu_count // 3)  # ì„œë¹„ìŠ¤ë³„ GPU ë¶„ë°°
                resources["requests"]["nvidia.com/gpu"] = str(gpu_count)
                resources["limits"]["nvidia.com/gpu"] = str(gpu_count)
            
            logger.info(
                "ë™ì  ë¦¬ì†ŒìŠ¤ ê³„ì‚° ì™„ë£Œ",
                service_name=service_name,
                total_channels=total_channels,
                scaling_factor=scaling_factor,
                final_resources=resources
            )
            
            return resources
            
        except Exception as e:
            logger.warning(
                "ë™ì  ë¦¬ì†ŒìŠ¤ ê³„ì‚° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©",
                service_name=service_name,
                error=str(e)
            )
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "requests": {"cpu": "500m", "memory": "1Gi"},
                "limits": {"cpu": "1000m", "memory": "2Gi"}
            }
    
    def _calculate_total_channels_from_specs(self, tenant_specs: TenantSpecs) -> int:
        """TenantSpecsì—ì„œ ì´ ì±„ë„ ìˆ˜ ê³„ì‚°"""
        try:
            total_channels = 0
            if hasattr(tenant_specs, 'services') and tenant_specs.services:
                total_channels += tenant_specs.services.get('callbot', 0)
                total_channels += tenant_specs.services.get('chatbot', 0)
                total_channels += tenant_specs.services.get('advisor', 0)
                total_channels += tenant_specs.services.get('stt', 0)
                total_channels += tenant_specs.services.get('tts', 0)
            return max(1, total_channels)  # ìµœì†Œ 1ì±„ë„
        except:
            return 100  # ê¸°ë³¸ê°’
    
    def _calculate_scaling_factor(self, total_channels: int, service_name: str) -> float:
        """ì„œë¹„ìŠ¤ë³„ ì±„ë„ ìˆ˜ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚°"""
        # ì„œë¹„ìŠ¤ë³„ ì±„ë„ë‹¹ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
        service_scaling = {
            "callbot": 0.8,   # ì½œë´‡ì€ ë†’ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©
            "chatbot": 0.3,   # ì±—ë´‡ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ
            "advisor": 0.6,   # ì–´ë“œë°”ì´ì €ëŠ” ì¤‘ê°„
            "tts": 1.0,       # TTSëŠ” ê°€ì¥ ë†’ìŒ
            "stt": 0.9,       # STTë„ ë†’ìŒ
            "ta": 0.4,        # TAëŠ” ì¤‘ê°„
            "qa": 0.2         # QAëŠ” ë‚®ìŒ
        }
        
        base_factor = service_scaling.get(service_name, 0.5)
        
        # ì±„ë„ ìˆ˜ì— ë”°ë¥¸ ê³„ë‹¨ì‹ ìŠ¤ì¼€ì¼ë§
        if total_channels <= 50:
            return base_factor * 1.0
        elif total_channels <= 200:
            return base_factor * 1.5
        elif total_channels <= 500:
            return base_factor * 2.0
        else:
            return base_factor * 3.0
    
    def _get_gpu_resource_multiplier(self, gpu_type) -> float:
        """GPU íƒ€ì…ë³„ ë¦¬ì†ŒìŠ¤ ë°°ìˆ˜"""
        gpu_multipliers = {
            "t4": 1.0,
            "v100": 1.3,
            "l40s": 1.6,
            "a100": 2.0
        }
        
        # Enum ì²˜ë¦¬
        if hasattr(gpu_type, 'value'):
            gpu_str = gpu_type.value.lower()
        else:
            gpu_str = str(gpu_type).lower()
            
        return gpu_multipliers.get(gpu_str, 1.0)
    
    def _generate_namespace(self, tenant_specs: TenantSpecs) -> str:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        template = Template(self.builtin_templates["namespace"])
        return template.render(
            tenant_id=tenant_specs.tenant_id,
            tenant_preset=tenant_specs.preset
        )
    
    def _generate_deployment(self, tenant_specs: TenantSpecs, service_name: str, advanced_config=None, image_info=None, resource_requirements=None) -> str:
        """Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± (ê³ ê¸‰ ì„¤ì • ì§€ì›)"""
        try:
            template = Template(self.builtin_templates["deployment"])
            
            # GPU í• ë‹¹ ì •ë³´ (ìƒˆë¡œìš´ TenantSpecs ëª¨ë¸ ê¸°ë°˜)
            has_gpu = service_name in ["tts", "nlp", "aicm"] and tenant_specs.gpu_count > 0
            
            # gpu_typeì´ enumì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if hasattr(tenant_specs.gpu_type, 'value'):
                gpu_type = tenant_specs.gpu_type.value
            else:
                gpu_type = str(tenant_specs.gpu_type)
            
            gpu_count = 1 if has_gpu else 0
            
            # [advice from AI] ì´ë¯¸ì§€ ì •ë³´ ì ìš©
            if image_info is None:
                image_info = self._get_service_image_info(service_name)
            
            # [advice from AI] ì±„ë„ ìˆ˜ ê¸°ë°˜ ë™ì  ë¦¬ì†ŒìŠ¤ ê³„ì‚°
            dynamic_resources = self._calculate_service_resources(tenant_specs, service_name, resource_requirements)
            
            # ê¸°ë³¸ container_specs ì„¤ì • (ì‹¤ì œ ì´ë¯¸ì§€ ì •ë³´ ë° ë™ì  ë¦¬ì†ŒìŠ¤ ì ìš©)
            container_specs = {
                "image": image_info["image"],
                "imagePullPolicy": image_info["pull_policy"],
                "ports": [{"containerPort": 8080, "protocol": "TCP"}],
                "env": [],
                "resources": dynamic_resources
            }
            
            # [advice from AI] ê³ ê¸‰ ì„¤ì • ì ìš©
            render_params = {
                "tenant_id": tenant_specs.tenant_id,
                "service_name": service_name,
                "replicas": 1,
                "has_gpu": has_gpu,
                "gpu_type": gpu_type,
                "gpu_count": gpu_count,
                "namespace": f"{tenant_specs.tenant_id}-ecp-ai",
                "container_specs": container_specs,
                "registry_secret": image_info.get("registry_secret")
            }
            
            if advanced_config:
                # ë¦¬ì†ŒìŠ¤ ì„¤ì •
                if hasattr(advanced_config, 'resource_config'):
                    resource_config = advanced_config.resource_config
                    render_params.update({
                        "resource_requests_cpu": resource_config.requests.cpu,
                        "resource_requests_memory": resource_config.requests.memory,
                        "resource_requests_ephemeral_storage": resource_config.requests.ephemeral_storage,
                        "resource_limits_cpu": resource_config.limits.cpu,
                        "resource_limits_memory": resource_config.limits.memory,
                        "resource_limits_ephemeral_storage": resource_config.limits.ephemeral_storage
                    })
                
                # í”„ë¡œë¸Œ ì„¤ì •
                if hasattr(advanced_config, 'latency_config'):
                    latency_config = advanced_config.latency_config
                    render_params.update({
                        "startup_probe_enabled": latency_config.startup_probe.enabled,
                        "startup_probe_initial_delay": latency_config.startup_probe.initial_delay_seconds,
                        "startup_probe_period": latency_config.startup_probe.period_seconds,
                        "startup_probe_timeout": latency_config.startup_probe.timeout_seconds,
                        "startup_probe_failure_threshold": latency_config.startup_probe.failure_threshold,
                        "startup_probe_success_threshold": latency_config.startup_probe.success_threshold,
                        "liveness_probe_initial_delay": latency_config.liveness_probe.initial_delay_seconds,
                        "liveness_probe_period": latency_config.liveness_probe.period_seconds,
                        "liveness_probe_timeout": latency_config.liveness_probe.timeout_seconds,
                        "liveness_probe_failure_threshold": latency_config.liveness_probe.failure_threshold,
                        "readiness_probe_initial_delay": latency_config.readiness_probe.initial_delay_seconds,
                        "readiness_probe_period": latency_config.readiness_probe.period_seconds,
                        "readiness_probe_timeout": latency_config.readiness_probe.timeout_seconds,
                        "readiness_probe_failure_threshold": latency_config.readiness_probe.failure_threshold,
                        "readiness_probe_success_threshold": latency_config.readiness_probe.success_threshold,
                        "rolling_update_max_surge": latency_config.rolling_update_max_surge,
                        "rolling_update_max_unavailable": latency_config.rolling_update_max_unavailable
                    })
            
            return template.render(**render_params)
        except Exception as e:
            logger.error("Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨", 
                        service_name=service_name, error=str(e))
            # ê¸°ë³¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë°˜í™˜
            return self._generate_basic_deployment(tenant_specs, service_name)
    
    def _generate_basic_deployment(self, tenant_specs: TenantSpecs, service_name: str) -> str:
        """ê¸°ë³¸ Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± (í…œí”Œë¦¿ ì˜¤ë¥˜ ì‹œ í´ë°±)"""
        # GPU í• ë‹¹ ì •ë³´ (ìƒˆë¡œìš´ TenantSpecs ëª¨ë¸ ê¸°ë°˜)
        has_gpu = service_name in ["tts", "nlp", "aicm"] and tenant_specs.gpu_count > 0
        
        basic_deployment = f"""---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {service_name}
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {service_name}
    tenant: {tenant_specs.tenant_id}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {service_name}
      tenant: {tenant_specs.tenant_id}
  template:
    metadata:
      labels:
        app: {service_name}
        tenant: {tenant_specs.tenant_id}
        monitoring: enabled
    spec:"""

        if has_gpu:
            # gpu_typeì´ enumì¸ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            if hasattr(tenant_specs.gpu_type, 'value'):
                gpu_type = tenant_specs.gpu_type.value
            else:
                gpu_type = str(tenant_specs.gpu_type)
                
            basic_deployment += f"""
      nodeSelector:
        accelerator: nvidia-{gpu_type}
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule"""

        basic_deployment += f"""
      containers:
      - name: {service_name}
        image: ecp-ai/{service_name}:latest
        ports:
        - containerPort: 8080
        env:
        - name: TENANT_ID
          value: {tenant_specs.tenant_id}
        - name: SERVICE_NAME
          value: {service_name}"""

        if has_gpu:
            basic_deployment += f"""
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: 1000m
            memory: 2Gi
          limits:
            nvidia.com/gpu: 1
            cpu: 4000m
            memory: 8Gi"""
        else:
            basic_deployment += f"""
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi"""

        basic_deployment += f"""
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
"""
        
        return basic_deployment
    
    def _should_create_gpu_service(self, tenant_specs: TenantSpecs, gpu_service: str) -> bool:
        """
        [advice from AI] GPU ì„œë¹„ìŠ¤ ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨
        """
        if gpu_service == "tts-server":
            # TTS ì±„ë„ì´ ìˆëŠ” ê²½ìš° (ì½œë´‡ + ë…ë¦½ TTS)
            return tenant_specs.total_channels > 0
        elif gpu_service == "nlp-server":
            # NLP ì²˜ë¦¬ê°€ í•„ìš”í•œ ì„œë¹„ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
            return tenant_specs.total_users > 0 or tenant_specs.total_channels > 0
        elif gpu_service == "aicm-server":
            # AICM ê²€ìƒ‰ì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
            return tenant_specs.total_channels > 0
        return False
    
    def _should_create_cpu_service(self, tenant_specs: TenantSpecs, cpu_service: str) -> bool:
        """
        [advice from AI] CPU ì„œë¹„ìŠ¤ ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨
        """
        if cpu_service == "stt-server":
            # STT ì²˜ë¦¬ê°€ í•„ìš”í•œ ì±„ë„ì´ ìˆëŠ” ê²½ìš°
            return tenant_specs.total_channels > 0
        elif cpu_service == "ta-server":
            # ë¶„ì„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
            return tenant_specs.total_channels > 0 or tenant_specs.total_users > 0
        elif cpu_service == "qa-server":
            # í’ˆì§ˆ ê´€ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°
            return tenant_specs.total_channels > 0 or tenant_specs.total_users > 0
        return False
    
    def _generate_gpu_deployment(self, tenant_specs: TenantSpecs, gpu_service: str) -> str:
        """
        [advice from AI] GPU ì „ìš© ì„œë¹„ìŠ¤ Deployment ìƒì„±
        """
        gpu_type_str = tenant_specs.gpu_type.value if hasattr(tenant_specs.gpu_type, 'value') else str(tenant_specs.gpu_type)
        
        # GPU ì„œë¹„ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì„¤ì •
        gpu_resources = {
            "tts-server": {"gpu_count": 1, "cpu": "2000m", "memory": "4Gi"},
            "nlp-server": {"gpu_count": 1, "cpu": "4000m", "memory": "8Gi"},
            "aicm-server": {"gpu_count": 1, "cpu": "2000m", "memory": "6Gi"}
        }
        
        resource_config = gpu_resources.get(gpu_service, {"gpu_count": 1, "cpu": "2000m", "memory": "4Gi"})
        
        return f"""---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {gpu_service}
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {gpu_service}
    tenant: {tenant_specs.tenant_id}
    tier: gpu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {gpu_service}
      tenant: {tenant_specs.tenant_id}
  template:
    metadata:
      labels:
        app: {gpu_service}
        tenant: {tenant_specs.tenant_id}
        tier: gpu
        monitoring: enabled
    spec:
      nodeSelector:
        accelerator: nvidia-{gpu_type_str}
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule
      containers:
      - name: {gpu_service}
        image: ecp-ai/{gpu_service}:latest
        ports:
        - containerPort: 8080
        env:
        - name: TENANT_ID
          value: {tenant_specs.tenant_id}
        - name: SERVICE_NAME
          value: {gpu_service}
        - name: GPU_TYPE
          value: {gpu_type_str}
        resources:
          requests:
            nvidia.com/gpu: {resource_config["gpu_count"]}
            cpu: {resource_config["cpu"]}
            memory: {resource_config["memory"]}
          limits:
            nvidia.com/gpu: {resource_config["gpu_count"]}
            cpu: {int(resource_config["cpu"].replace("m", "")) * 2}m
            memory: {int(resource_config["memory"].replace("Gi", "")) * 2}Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
"""
    
    def _generate_cpu_deployment(self, tenant_specs: TenantSpecs, cpu_service: str) -> str:
        """
        [advice from AI] CPU ì „ìš© ì„œë¹„ìŠ¤ Deployment ìƒì„±
        """
        # CPU ì„œë¹„ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì„¤ì •
        cpu_resources = {
            "stt-server": {"cpu": "4000m", "memory": "8Gi", "replicas": 2},
            "ta-server": {"cpu": "2000m", "memory": "4Gi", "replicas": 1},
            "qa-server": {"cpu": "1000m", "memory": "2Gi", "replicas": 1}
        }
        
        resource_config = cpu_resources.get(cpu_service, {"cpu": "1000m", "memory": "2Gi", "replicas": 1})
        
        return f"""---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {cpu_service}
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {cpu_service}
    tenant: {tenant_specs.tenant_id}
    tier: cpu
spec:
  replicas: {resource_config["replicas"]}
  selector:
    matchLabels:
      app: {cpu_service}
      tenant: {tenant_specs.tenant_id}
  template:
    metadata:
      labels:
        app: {cpu_service}
        tenant: {tenant_specs.tenant_id}
        tier: cpu
        monitoring: enabled
    spec:
      containers:
      - name: {cpu_service}
        image: ecp-ai/{cpu_service}:latest
        ports:
        - containerPort: 8080
        env:
        - name: TENANT_ID
          value: {tenant_specs.tenant_id}
        - name: SERVICE_NAME
          value: {cpu_service}
        resources:
          requests:
            cpu: {resource_config["cpu"]}
            memory: {resource_config["memory"]}
          limits:
            cpu: {int(resource_config["cpu"].replace("m", "")) * 2}m
            memory: {int(resource_config["memory"].replace("Gi", "")) * 2}Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
"""
    
    def _generate_infra_deployment(self, tenant_specs: TenantSpecs, infra_service: str) -> str:
        """
        [advice from AI] ì¸í”„ë¼ ì„œë¹„ìŠ¤ Deployment ìƒì„±
        """
        # ì¸í”„ë¼ ì„œë¹„ìŠ¤ë³„ ì„¤ì •
        infra_configs = {
            "nginx": {
                "image": "nginx:1.21-alpine",
                "port": 80,
                "cpu": "500m",
                "memory": "1Gi",
                "replicas": 2
            },
            "api-gateway": {
                "image": "ecp-ai/api-gateway:latest",
                "port": 8080,
                "cpu": "1000m",
                "memory": "2Gi",
                "replicas": 2
            },
            "postgresql": {
                "image": "postgres:13-alpine",
                "port": 5432,
                "cpu": "2000m",
                "memory": "4Gi",
                "replicas": 1
            },
            "auth-service": {
                "image": "ecp-ai/auth-service:latest",
                "port": 8080,
                "cpu": "500m",
                "memory": "1Gi",
                "replicas": 1
            },
            "vectordb": {
                "image": "qdrant/qdrant:latest",
                "port": 6333,
                "cpu": "1000m",
                "memory": "2Gi",
                "replicas": 1
            }
        }
        
        config = infra_configs.get(infra_service, infra_configs["nginx"])
        
        env_vars = ""
        if infra_service == "postgresql":
            env_vars = f"""
        - name: POSTGRES_DB
          value: {tenant_specs.tenant_id}_db
        - name: POSTGRES_USER
          value: ecp_user
        - name: POSTGRES_PASSWORD
          value: ecp_password"""
        elif infra_service == "auth-service":
            env_vars = f"""
        - name: JWT_SECRET
          value: ecp-jwt-secret-{tenant_specs.tenant_id}
        - name: DB_HOST
          value: postgresql-service"""
        
        return f"""---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {infra_service}
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {infra_service}
    tenant: {tenant_specs.tenant_id}
    tier: infrastructure
spec:
  replicas: {config["replicas"]}
  selector:
    matchLabels:
      app: {infra_service}
      tenant: {tenant_specs.tenant_id}
  template:
    metadata:
      labels:
        app: {infra_service}
        tenant: {tenant_specs.tenant_id}
        tier: infrastructure
        monitoring: enabled
    spec:
      containers:
      - name: {infra_service}
        image: {config["image"]}
        ports:
        - containerPort: {config["port"]}
        env:
        - name: TENANT_ID
          value: {tenant_specs.tenant_id}{env_vars}
        resources:
          requests:
            cpu: {config["cpu"]}
            memory: {config["memory"]}
          limits:
            cpu: {int(config["cpu"].replace("m", "")) * 2}m
            memory: {int(config["memory"].replace("Gi", "")) * 2}Gi
        livenessProbe:
          tcpSocket:
            port: {config["port"]}
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: {config["port"]}
          initialDelaySeconds: 10
          periodSeconds: 5
"""
    
    def _generate_gpu_hpa(self, tenant_specs: TenantSpecs, gpu_service: str) -> str:
        """
        [advice from AI] GPU ì„œë¹„ìŠ¤ìš© ì œí•œì  HPA ìƒì„±
        """
        return f"""---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {gpu_service}-hpa
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {gpu_service}
    tenant: {tenant_specs.tenant_id}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {gpu_service}
  minReplicas: 1
  maxReplicas: 3  # GPU ì„œë¹„ìŠ¤ëŠ” ì œí•œì  ìŠ¤ì¼€ì¼ë§
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80  # GPU ì„œë¹„ìŠ¤ëŠ” ë†’ì€ ì„ê³„ê°’
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300  # ë” ê¸´ ì•ˆì •í™” ì‹œê°„
      policies:
      - type: Percent
        value: 50
        periodSeconds: 300
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
      - type: Percent
        value: 25
        periodSeconds: 300
"""
    
    def _generate_storage_pvc(self, tenant_specs: TenantSpecs) -> str:
        """
        [advice from AI] ìŠ¤í† ë¦¬ì§€ PVC ìƒì„±
        """
        storage_size = f"{max(100, tenant_specs.storage_gb)}Gi"
        
        return f"""---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ecp-storage-pvc
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    tenant: {tenant_specs.tenant_id}
    tier: storage
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: {storage_size}
  storageClassName: fast-ssd
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ecp-logs-pvc
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    tenant: {tenant_specs.tenant_id}
    tier: storage
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: standard
"""
    
    def _generate_service(self, tenant_specs: TenantSpecs, service_name: str) -> str:
        """Service ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        try:
            template = Template(self.builtin_templates["service"])
            
            # ê¸°ë³¸ í¬íŠ¸ ì„¤ì •
            ports = [{"containerPort": 8080}]
            
            return template.render(
                tenant_id=tenant_specs.tenant_id,
                service_name=service_name,
                ports=ports,
                namespace=f"{tenant_specs.tenant_id}-ecp-ai"
            )
        except Exception as e:
            logger.error("Service ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨", 
                        service_name=service_name, error=str(e))
            # ê¸°ë³¸ Service ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë°˜í™˜
            return f"""---
apiVersion: v1
kind: Service
metadata:
  name: {service_name}-service
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    app: {service_name}
    tenant: {tenant_specs.tenant_id}
    monitoring: enabled
spec:
  selector:
    app: {service_name}
    tenant: {tenant_specs.tenant_id}
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP
  type: ClusterIP
"""
    
    def _generate_hpa(self, tenant_specs: TenantSpecs, service_name: str, advanced_config=None) -> str:
        """HPA ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± (ê³ ê¸‰ ì„¤ì • ì§€ì›)"""
        template = Template(self.builtin_templates["hpa"])
        
        # ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§ ì„¤ì •
        render_params = {
            "tenant_id": tenant_specs.tenant_id,
            "service_name": service_name,
            "min_replicas": 1,
            "max_replicas": 10,
            "target_cpu": 70,
            "target_memory": 80,
            "namespace": f"{tenant_specs.tenant_id}-ecp-ai",
            "custom_metrics_enabled": False,
            "custom_metric_name": "",
            "custom_target_value": 100,
            "scale_up_stabilization_window": 60,
            "scale_up_max_percent": 100,
            "scale_up_period_seconds": 60,
            "scale_down_stabilization_window": 300,
            "scale_down_max_percent": 10,
            "scale_down_period_seconds": 60
        }
        
        # [advice from AI] ê³ ê¸‰ ì„¤ì • ì ìš©
        if advanced_config and hasattr(advanced_config, 'auto_scaling'):
            auto_scaling = advanced_config.auto_scaling
            if auto_scaling.enabled:
                render_params.update({
                    "min_replicas": auto_scaling.min_replicas,
                    "max_replicas": auto_scaling.max_replicas,
                    "target_cpu": auto_scaling.target_cpu,
                    "target_memory": auto_scaling.target_memory,
                    "custom_metrics_enabled": auto_scaling.custom_metrics_enabled,
                    "custom_metric_name": auto_scaling.custom_metric_name or "",
                    "custom_target_value": auto_scaling.custom_target_value or 100,
                    "scale_up_stabilization_window": auto_scaling.scale_up_stabilization_window,
                    "scale_up_max_percent": auto_scaling.scale_up_max_percent,
                    "scale_up_period_seconds": auto_scaling.scale_up_period_seconds,
                    "scale_down_stabilization_window": auto_scaling.scale_down_stabilization_window,
                    "scale_down_max_percent": auto_scaling.scale_down_max_percent,
                    "scale_down_period_seconds": auto_scaling.scale_down_period_seconds
                })
        
        return template.render(**render_params)
    
    def _generate_configmap(self, tenant_specs: TenantSpecs) -> str:
        """ConfigMap ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        try:
            template = Template(self.builtin_templates["configmap"])
            
            return template.render(
                tenant_id=tenant_specs.tenant_id,
                tenant_specs=tenant_specs,
                namespace=f"{tenant_specs.tenant_id}-ecp-ai"
            )
        except Exception as e:
            logger.error("ConfigMap ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨", error=str(e))
            # gpu_typeê³¼ presetì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            gpu_type_str = tenant_specs.gpu_type.value if hasattr(tenant_specs.gpu_type, 'value') else str(tenant_specs.gpu_type)
            preset_str = tenant_specs.preset.value if hasattr(tenant_specs.preset, 'value') else str(tenant_specs.preset)
            # ê¸°ë³¸ ConfigMap ë°˜í™˜
            return f"""---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ecp-config
  namespace: {tenant_specs.tenant_id}-ecp-ai
  labels:
    tenant: {tenant_specs.tenant_id}
data:
  tenant-config.yaml: |
    tenantId: {tenant_specs.tenant_id}
    preset: {preset_str}
    services: {{}}
    resources:
      gpu_type: {gpu_type_str}
      auto_scaling: true
"""
    
    def _generate_networkpolicy(self, tenant_specs: TenantSpecs) -> str:
        """NetworkPolicy ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        template = Template(self.builtin_templates["networkpolicy"])
        
        return template.render(
            tenant_id=tenant_specs.tenant_id,
            namespace=f"{tenant_specs.tenant_id}-ecp-ai"
        )
    
    def _generate_monitoring(self, tenant_specs: TenantSpecs) -> str:
        """ëª¨ë‹ˆí„°ë§ ì„¤ì • ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        monitoring_yaml = {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "ServiceMonitor",
            "metadata": {
                "name": "ecp-ai-monitoring",
                "namespace": f"{tenant_specs.tenant_id}-ecp-ai",
                "labels": {
                    "tenant": tenant_specs.tenant_id,
                    "app.kubernetes.io/name": "ecp-ai"
                }
            },
            "spec": {
                "selector": {
                    "matchLabels": {
                        "monitoring": "enabled"
                    }
                },
                "endpoints": [
                    {
                        "port": "metrics",
                        "interval": "30s",
                        "path": "/metrics"
                    }
                ]
            }
        }
        
        return yaml.dump(monitoring_yaml, default_flow_style=False, allow_unicode=True)
    
    def _generate_deploy_script(self, tenant_specs: TenantSpecs) -> str:
        """ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        # gpu_typeê³¼ presetì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        gpu_type_str = tenant_specs.gpu_type.value if hasattr(tenant_specs.gpu_type, 'value') else str(tenant_specs.gpu_type)
        preset_str = tenant_specs.preset.value if hasattr(tenant_specs.preset, 'value') else str(tenant_specs.preset)
        
        return f"""#!/bin/bash
# [advice from AI] ECP-AI í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸš€ ECP-AI í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ë°°í¬ ì‹œì‘"
echo "í”„ë¦¬ì…‹: {preset_str}"
echo "GPU íƒ€ì…: {gpu_type_str}"
echo "ì˜ˆìƒ ë¦¬ì†ŒìŠ¤: GPU {tenant_specs.gpu_count}ê°œ, CPU {tenant_specs.cpu_cores}ì½”ì–´"
echo ""

# 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
echo "ğŸ“ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì¤‘..."
kubectl apply -f k8s-manifests/01-namespace.yaml

# 2. ConfigMap ìƒì„±
echo "âš™ï¸ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘..."
kubectl apply -f k8s-manifests/02-configmap.yaml

# 3. ì„œë¹„ìŠ¤ë³„ ë°°í¬
echo "ğŸ› ï¸ ì„œë¹„ìŠ¤ ë°°í¬ ì¤‘..."
for manifest in k8s-manifests/*-deployment.yaml; do
    if [ -f "$manifest" ]; then
        echo "ë°°í¬ ì¤‘: $(basename $manifest)"
        kubectl apply -f "$manifest"
    fi
done

for manifest in k8s-manifests/*-service.yaml; do
    if [ -f "$manifest" ]; then
        echo "ì„œë¹„ìŠ¤ ìƒì„±: $(basename $manifest)"
        kubectl apply -f "$manifest"
    fi
done

# 4. ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì„¤ì •
echo "ğŸ“ˆ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì„¤ì • ì¤‘..."
for manifest in k8s-manifests/*-hpa.yaml; do
    if [ -f "$manifest" ]; then
        echo "HPA ìƒì„±: $(basename $manifest)"
        kubectl apply -f "$manifest"
    fi
done

# 5. ë„¤íŠ¸ì›Œí¬ ì •ì±… ë° ëª¨ë‹ˆí„°ë§
echo "ğŸ”’ ë³´ì•ˆ ì •ì±… ì„¤ì • ì¤‘..."
kubectl apply -f k8s-manifests/90-networkpolicy.yaml
kubectl apply -f k8s-manifests/91-monitoring.yaml

echo ""
echo "âœ… í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ë°°í¬ ì™„ë£Œ!"
echo ""
echo "ğŸ“Š ìƒíƒœ í™•ì¸:"
echo "  kubectl get pods -n {tenant_specs.tenant_id}-ecp-ai"
echo "  kubectl get services -n {tenant_specs.tenant_id}-ecp-ai"
echo "  kubectl get hpa -n {tenant_specs.tenant_id}-ecp-ai"
echo ""
echo "ğŸ” ë¡œê·¸ í™•ì¸:"
echo "  kubectl logs -f deployment/<service-name> -n {tenant_specs.tenant_id}-ecp-ai"
echo ""
echo "ğŸ—‘ï¸ ì‚­ì œ ë°©ë²•:"
echo "  ./cleanup.sh"
"""

    def _generate_cleanup_script(self, tenant_specs: TenantSpecs) -> str:
        """ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        return f"""#!/bin/bash
# [advice from AI] ECP-AI í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ğŸ—‘ï¸ ECP-AI í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ì‚­ì œ ì‹œì‘"
echo ""

read -p "ì •ë§ë¡œ í…Œë„Œì‹œ '{tenant_specs.tenant_id}'ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "ì‚­ì œ ì·¨ì†Œë¨"
    exit 0
fi

echo "ğŸ§¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì‚­ì œ ì¤‘..."
kubectl delete namespace {tenant_specs.tenant_id}-ecp-ai --ignore-not-found=true

echo ""
echo "âœ… í…Œë„Œì‹œ '{tenant_specs.tenant_id}' ì‚­ì œ ì™„ë£Œ!"
echo ""
echo "ğŸ“Š í™•ì¸:"
echo "  kubectl get namespaces | grep {tenant_specs.tenant_id}"
"""

    def _generate_readme(self, tenant_specs: TenantSpecs) -> str:
        """README íŒŒì¼ ìƒì„±"""
        # ê¸°ë³¸ ì„œë¹„ìŠ¤ ëª©ë¡
        basic_services = ["callbot", "chatbot", "advisor", "stt", "tts", "ta", "qa"]
        services_list = "\n".join([
            f"- **{name}**: 1ê°œ ì¸ìŠ¤í„´ìŠ¤"
            for name in basic_services
        ])
        
        # gpu_typeê³¼ presetì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        gpu_type_str = tenant_specs.gpu_type.value if hasattr(tenant_specs.gpu_type, 'value') else str(tenant_specs.gpu_type)
        preset_str = tenant_specs.preset.value if hasattr(tenant_specs.preset, 'value') else str(tenant_specs.preset)
        
        return f"""# ECP-AI í…Œë„Œì‹œ: {tenant_specs.tenant_id}

## ğŸ“‹ í…Œë„Œì‹œ ì •ë³´

- **í…Œë„Œì‹œ ID**: {tenant_specs.tenant_id}
- **í”„ë¦¬ì…‹**: {preset_str}
- **GPU íƒ€ì…**: {gpu_type_str}
- **ì˜ˆìƒ ë¦¬ì†ŒìŠ¤**: GPU {tenant_specs.gpu_count}ê°œ, CPU {tenant_specs.cpu_cores}ì½”ì–´

## ğŸ› ï¸ ì„œë¹„ìŠ¤ êµ¬ì„±

{services_list}

## ğŸš€ ë°°í¬ ë°©ë²•

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­
- Kubernetes í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ ê¶Œí•œ
- kubectl ëª…ë ¹ì–´ ì„¤ì¹˜
- ì¶©ë¶„í•œ í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤

### 2. ë°°í¬ ì‹¤í–‰
```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x deploy.sh cleanup.sh

# ë°°í¬ ì‹¤í–‰
./deploy.sh
```

### 3. ìƒíƒœ í™•ì¸
```bash
# Pod ìƒíƒœ í™•ì¸
kubectl get pods -n {tenant_specs.tenant_id}-ecp-ai

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
kubectl get services -n {tenant_specs.tenant_id}-ecp-ai

# HPA ìƒíƒœ í™•ì¸
kubectl get hpa -n {tenant_specs.tenant_id}-ecp-ai
```

### 4. ë¡œê·¸ í™•ì¸
```bash
# ì „ì²´ Pod ë¡œê·¸
kubectl logs -f -l tenant={tenant_specs.tenant_id} -n {tenant_specs.tenant_id}-ecp-ai

# íŠ¹ì • ì„œë¹„ìŠ¤ ë¡œê·¸
kubectl logs -f deployment/<service-name> -n {tenant_specs.tenant_id}-ecp-ai
```

## ğŸ—‘ï¸ ì‚­ì œ ë°©ë²•

```bash
# ì „ì²´ í…Œë„Œì‹œ ì‚­ì œ
./cleanup.sh

# ë˜ëŠ” ìˆ˜ë™ ì‚­ì œ
kubectl delete namespace {tenant_specs.tenant_id}-ecp-ai
```

## ğŸ“Š ì˜ˆìƒ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

### GPU ìš”êµ¬ì‚¬í•­
- **GPU íƒ€ì…**: {gpu_type_str}
- **GPU ê°œìˆ˜**: {tenant_specs.gpu_count}ê°œ
- **GPU ë©”ëª¨ë¦¬**: {tenant_specs.memory_gb}GB

### CPU ìš”êµ¬ì‚¬í•­  
- **CPU ì½”ì–´**: {tenant_specs.cpu_cores}ê°œ
- **ë©”ëª¨ë¦¬**: {tenant_specs.memory_gb}GB
- **ìŠ¤í† ë¦¬ì§€**: {tenant_specs.storage_gb}GB

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë¦¬ì†ŒìŠ¤ í™•ì¸**: í´ëŸ¬ìŠ¤í„°ì— ì¶©ë¶„í•œ GPU/CPU ë¦¬ì†ŒìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
2. **ë„¤íŠ¸ì›Œí¬ ì •ì±…**: ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ ì •ì±…ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
3. **ëª¨ë‹ˆí„°ë§**: Prometheus Operatorê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ ëª¨ë‹ˆí„°ë§ì´ ì‘ë™í•©ë‹ˆë‹¤
4. **ìŠ¤í† ë¦¬ì§€**: PersistentVolumeì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ECP-AI ì§€ì›íŒ€ì— ë¬¸ì˜í•˜ì„¸ìš”.
- ì´ë©”ì¼: support@ecp-ai.com
- ë¬¸ì„œ: https://docs.ecp-ai.com
"""

    # í…œí”Œë¦¿ ì •ì˜ë“¤
    def _get_namespace_template(self) -> str:
        return """---
apiVersion: v1
kind: Namespace
metadata:
  name: {{ tenant_id }}-ecp-ai
  labels:
    tenant: {{ tenant_id }}
    app.kubernetes.io/name: ecp-ai
    app.kubernetes.io/managed-by: ecp-orchestrator
    tier: {{ tenant_preset }}
    ecp.ai/tenant-id: {{ tenant_id }}
    ecp.ai/preset: {{ tenant_preset }}
  annotations:
    ecp.ai/created-by: ecp-orchestrator
    ecp.ai/description: "ECP-AI tenant namespace for {{ tenant_id }}"
"""

    def _get_deployment_template(self) -> str:
        return """---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ service_name }}
  namespace: {{ namespace }}
  labels:
    app: {{ service_name }}
    tenant: {{ tenant_id }}
    ecp.ai/tenant-id: {{ tenant_id }}
    ecp.ai/service: {{ service_name }}
spec:
  replicas: {{ replicas }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: {{ rolling_update_max_surge | default('25%') }}
      maxUnavailable: {{ rolling_update_max_unavailable | default('25%') }}
  selector:
    matchLabels:
      app: {{ service_name }}
      tenant: {{ tenant_id }}
  template:
    metadata:
      labels:
        app: {{ service_name }}
        tenant: {{ tenant_id }}
        ecp.ai/tenant-id: {{ tenant_id }}
        ecp.ai/service: {{ service_name }}
        monitoring: enabled
    spec:
      {% if registry_secret %}
      imagePullSecrets:
      - name: {{ registry_secret }}
      {% endif %}
      {% if has_gpu %}
      nodeSelector:
        accelerator: nvidia-{{ gpu_type }}
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: present
        effect: NoSchedule
      {% endif %}
      containers:
      - name: {{ service_name }}
        image: {{ container_specs.image }}
        ports:
        {% for port in container_specs.ports %}
        - containerPort: {{ port.containerPort }}
          protocol: {{ port.get('protocol', 'TCP') }}
        {% endfor %}
        env:
        - name: TENANT_ID
          value: {{ tenant_id }}
        - name: SERVICE_NAME
          value: {{ service_name }}
        {% if has_gpu %}
        - name: GPU_TYPE
          value: {{ gpu_type }}
        {% endif %}
        {% for env_var in container_specs.get('env', []) %}
        - name: {{ env_var.name }}
          value: {{ env_var.value }}
        {% endfor %}
        resources:
          requests:
            {% if has_gpu %}
            nvidia.com/gpu: {{ gpu_count }}
            {% endif %}
            cpu: {{ resource_requests_cpu | default('100m') }}
            memory: {{ resource_requests_memory | default('256Mi') }}
            {% if resource_requests_ephemeral_storage %}
            ephemeral-storage: {{ resource_requests_ephemeral_storage }}
            {% endif %}
          limits:
            {% if has_gpu %}
            nvidia.com/gpu: {{ gpu_count }}
            {% endif %}
            cpu: {{ resource_limits_cpu | default('1000m') }}
            memory: {{ resource_limits_memory | default('1Gi') }}
            {% if resource_limits_ephemeral_storage %}
            ephemeral-storage: {{ resource_limits_ephemeral_storage }}
            {% endif %}
        {% if startup_probe_enabled %}
        startupProbe:
          httpGet:
            path: /startup
            port: {{ container_specs.ports[0].containerPort }}
          initialDelaySeconds: {{ startup_probe_initial_delay }}
          periodSeconds: {{ startup_probe_period }}
          timeoutSeconds: {{ startup_probe_timeout }}
          failureThreshold: {{ startup_probe_failure_threshold }}
          successThreshold: {{ startup_probe_success_threshold }}
        {% endif %}
        livenessProbe:
          httpGet:
            path: /health
            port: {{ container_specs.ports[0].containerPort }}
          initialDelaySeconds: {{ liveness_probe_initial_delay | default(30) }}
          periodSeconds: {{ liveness_probe_period | default(10) }}
          timeoutSeconds: {{ liveness_probe_timeout | default(5) }}
          failureThreshold: {{ liveness_probe_failure_threshold | default(3) }}
        readinessProbe:
          httpGet:
            path: /ready
            port: {{ container_specs.ports[0].containerPort }}
          initialDelaySeconds: {{ readiness_probe_initial_delay | default(5) }}
          periodSeconds: {{ readiness_probe_period | default(5) }}
          timeoutSeconds: {{ readiness_probe_timeout | default(3) }}
          failureThreshold: {{ readiness_probe_failure_threshold | default(3) }}
          successThreshold: {{ readiness_probe_success_threshold | default(1) }}
"""

    def _get_service_template(self) -> str:
        return """---
apiVersion: v1
kind: Service
metadata:
  name: {{ service_name }}-service
  namespace: {{ namespace }}
  labels:
    app: {{ service_name }}
    tenant: {{ tenant_id }}
    ecp.ai/tenant-id: {{ tenant_id }}
    ecp.ai/service: {{ service_name }}
    monitoring: enabled
spec:
  selector:
    app: {{ service_name }}
    tenant: {{ tenant_id }}
  ports:
  {% for port in ports %}
  - name: port-{{ port.containerPort }}
    port: {{ port.containerPort }}
    targetPort: {{ port.containerPort }}
    protocol: {{ port.get('protocol', 'TCP') }}
  {% endfor %}
  type: ClusterIP
"""

    def _get_hpa_template(self) -> str:
        return """---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{ service_name }}-hpa
  namespace: {{ namespace }}
  labels:
    app: {{ service_name }}
    tenant: {{ tenant_id }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ service_name }}
  minReplicas: {{ min_replicas }}
  maxReplicas: {{ max_replicas }}
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{ target_cpu }}
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{ target_memory }}
  {% if custom_metrics_enabled %}
  - type: Pods
    pods:
      metric:
        name: {{ custom_metric_name }}
      target:
        type: AverageValue
        averageValue: {{ custom_target_value }}
  {% endif %}
  behavior:
    scaleUp:
      stabilizationWindowSeconds: {{ scale_up_stabilization_window }}
      policies:
      - type: Percent
        value: {{ scale_up_max_percent }}
        periodSeconds: {{ scale_up_period_seconds }}
    scaleDown:
      stabilizationWindowSeconds: {{ scale_down_stabilization_window }}
      policies:
      - type: Percent
        value: {{ scale_down_max_percent }}
        periodSeconds: {{ scale_down_period_seconds }}
"""

    def _get_configmap_template(self) -> str:
        return """---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ecp-config
  namespace: {{ namespace }}
  labels:
    tenant: {{ tenant_id }}
    ecp.ai/tenant-id: {{ tenant_id }}
data:
  tenant-config.yaml: |
    tenantId: {{ tenant_id }}
    preset: {{ tenant_specs.preset }}
    services:
      callbot:
        enabled: true
        count: 1
      chatbot:
        enabled: true
        count: 1
      advisor:
        enabled: true
        count: 1
      stt:
        enabled: true
        count: 1
      tts:
        enabled: true
        count: 1
      ta:
        enabled: true
        count: 1
      qa:
        enabled: true
        count: 1
    resources:
      gpu_type: {{ tenant_specs.gpu_type.value if tenant_specs.gpu_type.value is defined else tenant_specs.gpu_type }}
      gpu_count: {{ tenant_specs.gpu_count }}
      cpu_cores: {{ tenant_specs.cpu_cores }}
    sla:
      availability: 99.9
      response_time: 300
"""

    def _get_networkpolicy_template(self) -> str:
        return """---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ecp-ai-network-policy
  namespace: {{ namespace }}
  labels:
    tenant: {{ tenant_id }}
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          ecp.ai/tenant-id: {{ tenant_id }}
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          ecp.ai/tenant-id: {{ tenant_id }}
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
"""
