import json
import math
import logging
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class ResourceCalculation:
    """ë¦¬ì†ŒìŠ¤ ê³„ì‚° ê²°ê³¼"""
    gpu: Dict[str, float]
    cpu: Dict[str, float] 
    network: Dict[str, float]
    storage: Dict[str, float]
    infrastructure: Dict[str, float]
    # ì¶”ê°€ ì†ì„±ë“¤
    stt_channels: float = 0
    tts_channels: float = 0
    ta_channels: float = 0
    qa_channels: float = 0
    stt_breakdown: Dict[str, int] = None
    nlp_breakdown: Dict[str, int] = None
    aicm_breakdown: Dict[str, int] = None
    infra_breakdown: Dict = None

@dataclass
class HardwareSpecification:
    """í•˜ë“œì›¨ì–´ ì‚¬ì–‘"""
    gpu_servers: List[Dict]
    cpu_servers: List[Dict]
    storage_servers: List[Dict]
    infrastructure_servers: List[Dict]
    network_requirements: str
    infrastructure_notes: str

class ECPHardwareCalculator:
    """ECP-AI í†µí•© í•˜ë“œì›¨ì–´ ê³„ì‚°ê¸°"""
    
    def __init__(self, config_path: str = "config"):
        self.config_path = Path(config_path)
        self.service_specs = self._load_json("service_specs.json")
        self.gpu_capacity = self._load_json("gpu_capacity.json") 
        self.correlation_matrix = self._load_json("correlation_matrix.json")
        self.infrastructure = self._load_json("infrastructure.json")
        
    def _load_json(self, filename: str) -> Dict[str, Any]:
        """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path / filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"{filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def calculate_hardware_requirements(self, 
                                      requirements: Dict[str, int],
                                      gpu_type: str = "t4") -> Tuple[ResourceCalculation, HardwareSpecification]:
        """
        í†µí•© í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
        
        Args:
            requirements: {"callbot": 30, "chatbot": 30, "advisor": 20, ...}
            gpu_type: "t4", "v100", "l40s"
            
        Returns:
            ResourceCalculation, HardwareSpecification
        """
        
        # ì „ì²´ ìš”êµ¬ì‚¬í•­ ì§ì ‘ ì‚¬ìš©
        total_req = requirements
        
        # ì„œë¹„ìŠ¤ë³„ ë¶€í•˜ ë¶„ì„
        service_loads = self._analyze_service_loads(total_req)
        
        # ë¦¬ì†ŒìŠ¤ ê³„ì‚°
        infrastructure_reqs = self._calculate_infrastructure_requirements(total_req)
        cpu_reqs = self._calculate_cpu_requirements(service_loads)
        
        # ì¸í”„ë¼ CPU ì´í•© ê³„ì‚°
        total_infra_cpu = sum(infrastructure_reqs.values())
        
        # CPU ì´í•©ì— ì‹¤ì œ ì¸í”„ë¼ CPU ë°˜ì˜
        cpu_reqs['infrastructure'] = total_infra_cpu
        cpu_reqs['total'] = cpu_reqs['stt'] + cpu_reqs['ta'] + cpu_reqs['qa'] + total_infra_cpu
        
        # ì´ ì±„ë„ ìˆ˜ ê³„ì‚° (ë™ì  ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´)
        total_channels = sum(v for k, v in requirements.items() if isinstance(v, (int, float)) and k != 'gpu_type')
        
        resources = ResourceCalculation(
            gpu=self._calculate_gpu_requirements(service_loads, gpu_type, total_channels, requirements),
            cpu=cpu_reqs,
            network=self._calculate_network_requirements(service_loads),
            storage=self._calculate_storage_requirements(service_loads),
            infrastructure=infrastructure_reqs,
            # ì±„ë„ ì •ë³´ ì¶”ê°€
            stt_channels=service_loads['stt_channels'],
            tts_channels=service_loads['tts_channels'],
            ta_channels=service_loads['ta_daily_processing'],
            qa_channels=service_loads['qa_daily_evaluations'],
            stt_breakdown=service_loads['stt_breakdown'],
            nlp_breakdown=service_loads['nlp_breakdown'],
            aicm_breakdown=service_loads['aicm_breakdown'],
            infra_breakdown=service_loads['infra_breakdown']
        )
        
        # í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ìƒì„±
        hardware_spec = self._generate_hardware_specification(resources, total_req, total_channels)
        
        # ì‹¤ì œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°
        actual_memory_gb = self._calculate_actual_memory_requirements(hardware_spec)
        
        return resources, hardware_spec
    

    
    def _analyze_service_loads(self, requirements: Dict[str, int]) -> Dict[str, float]:
        """ì„œë¹„ìŠ¤ë³„ ì¼ì¼ ì²˜ë¦¬ëŸ‰ ë¶„ì„"""
        service_loads = {
            'stt_channels': 0,
            'tts_channels': 0,
            'nlp_daily_queries': 0,
            'aicm_daily_queries': 0,
            'ta_daily_processing': 0,
            'qa_daily_evaluations': 0,
            # ì„œë¹„ìŠ¤ë³„ ì¢…ì†ì„± ì¶”ì 
            'stt_breakdown': {
                'callbot': 0,
                'chatbot': 0,
                'advisor': 0,
                'standalone': 0
            },
            'nlp_breakdown': {
                'callbot': 0,
                'chatbot': 0,
                'advisor': 0,
                'ta': 0
            },
            'aicm_breakdown': {
                'callbot': 0,
                'chatbot': 0,
                'advisor': 0
            },
            'infra_breakdown': {
                'total_channels': 0,
                'services': [],
                'primary_services': []
            }
        }
        
        # ë©”ì¸ ì„œë¹„ìŠ¤ ì²˜ë¦¬ (ê°€ì´ë“œ ê¸°ì¤€ ì •í™•í•œ ìˆ˜ì¹˜ ì ìš©)
        if requirements.get('callbot', 0) > 0:
            callbot_stt = requirements['callbot'] * 1  # ì½œë´‡ 1ì±„ë„ë‹¹ STT 1ì±„ë„ (í™˜ê²½ì„¤ì • ê¸°ì¤€)
            callbot_nlp = requirements['callbot'] * 3200  # 160ì½œ Ã— 20ì¿¼ë¦¬ = 3,200ì¿¼ë¦¬/ì¼
            callbot_aicm = requirements['callbot'] * 480   # 160ì½œ Ã— 3ì¿¼ë¦¬ = 480ì¿¼ë¦¬/ì¼
            
            service_loads['stt_channels'] += callbot_stt
            service_loads['stt_breakdown']['callbot'] = callbot_stt
            service_loads['tts_channels'] += requirements['callbot']  # ì½œë´‡ 1ê°œë‹¹ TTS 1ì±„ë„ (ì½œë´‡ë§Œ TTS ì‚¬ìš©)
            service_loads['nlp_daily_queries'] += callbot_nlp
            service_loads['nlp_breakdown']['callbot'] = callbot_nlp
            service_loads['aicm_daily_queries'] += callbot_aicm
            service_loads['aicm_breakdown']['callbot'] = callbot_aicm
        
        if requirements.get('chatbot', 0) > 0:
            # ì±—ë´‡ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„œë¹„ìŠ¤ë¡œ STT ë¶ˆí•„ìš” (24ì‹œê°„ ìš´ì˜ ê³ ë ¤)
            chatbot_nlp = requirements['chatbot'] * 96  # 8ì„¸ì…˜ Ã— 12ì¿¼ë¦¬ = 96ì¿¼ë¦¬/ì¼
            chatbot_aicm = requirements['chatbot'] * 9.6  # 8ì„¸ì…˜ Ã— 1.2ì¿¼ë¦¬ = 9.6ì¿¼ë¦¬/ì¼
            
            # ì±—ë´‡ì€ STTì™€ TTS ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            service_loads['nlp_daily_queries'] += chatbot_nlp
            service_loads['nlp_breakdown']['chatbot'] = chatbot_nlp
            service_loads['aicm_daily_queries'] += chatbot_aicm
            service_loads['aicm_breakdown']['chatbot'] = chatbot_aicm
        
        if requirements.get('advisor', 0) > 0:
            advisor_stt = requirements['advisor'] * 2  # ì–´ë“œë°”ì´ì € 1ëª…ë‹¹ STT 2ì±„ë„
            advisor_nlp = requirements['advisor'] * 2400  # 160ìƒë‹´ Ã— 15ì¿¼ë¦¬ = 2,400ì¿¼ë¦¬/ì¼
            advisor_aicm = requirements['advisor'] * 1360  # 160ìƒë‹´ Ã— 8.5ì¿¼ë¦¬ = 1,360ì¿¼ë¦¬/ì¼
            
            service_loads['stt_channels'] += advisor_stt
            service_loads['stt_breakdown']['advisor'] = advisor_stt
            # ì–´ë“œë°”ì´ì €ëŠ” TTS ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            service_loads['nlp_daily_queries'] += advisor_nlp
            service_loads['nlp_breakdown']['advisor'] = advisor_nlp
            service_loads['aicm_daily_queries'] += advisor_aicm
            service_loads['aicm_breakdown']['advisor'] = advisor_aicm
        
        # ë…ë¦½ ì„œë¹„ìŠ¤ ì²˜ë¦¬ (ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ê²½ìš°ë§Œ)
        if requirements.get('stt', 0) > 0:
            standalone_stt = requirements['stt']  # ë…ë¦½ STT ì±„ë„
            service_loads['stt_channels'] += standalone_stt
            service_loads['stt_breakdown']['standalone'] = standalone_stt
        
        if requirements.get('tts', 0) > 0:
            service_loads['tts_channels'] += requirements['tts']  # ë…ë¦½ TTS ì±„ë„
        
        if requirements.get('ta', 0) > 0:
            # TAëŠ” ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìœ íœ´ì‹œê°„ í™œìš©í•˜ì§€ë§Œ ê¸°ì¡´ ëŒ€í™” ì¬ë¶„ì„ì„ ìœ„í•œ NLP ì²˜ë¦¬ í•„ìš”
            # ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤ì˜ NLP ì²˜ë¦¬ëŸ‰ì˜ 30%ë¥¼ ì¶”ê°€ ë¶€í•˜ë¡œ ê³„ì‚°
            service_loads['ta_daily_processing'] += requirements['ta']  # TA ì±„ë„ ìˆ˜
            
            # ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤ì˜ ì´ NLP ì¿¼ë¦¬ ìˆ˜ ê³„ì‚°
            existing_nlp_queries = (
                service_loads['nlp_breakdown'].get('callbot', 0) +
                service_loads['nlp_breakdown'].get('chatbot', 0) +
                service_loads['nlp_breakdown'].get('advisor', 0)
            )
            
            # TA NLP ë¶€í•˜: ê¸°ì¡´ NLP ì²˜ë¦¬ëŸ‰ì˜ 30% (ë°°ì¹˜ ì²˜ë¦¬ íŠ¹ì„± ë°˜ì˜)
            ta_nlp_load_factor = 0.3  # 30% ì˜í–¥ë„
            ta_nlp = existing_nlp_queries * ta_nlp_load_factor  # TA ì±„ë„ ìˆ˜ ì¤‘ë³µ ê³±ì…ˆ ì œê±°
            
            service_loads['nlp_daily_queries'] += ta_nlp
            service_loads['nlp_breakdown']['ta'] = ta_nlp
        
        if requirements.get('qa', 0) > 0:
            service_loads['qa_daily_evaluations'] += requirements['qa'] * 200  # QA 1ê°œë‹¹ ì¼ì¼ 200ê±´ í‰ê°€
        
        # ì¸í”„ë¼ breakdown ì •ë³´ ìˆ˜ì§‘
        total_channels = sum([
            requirements.get('callbot', 0),
            requirements.get('chatbot', 0), 
            requirements.get('advisor', 0),
            requirements.get('ta', 0),
            requirements.get('qa', 0),
            requirements.get('stt', 0),
            requirements.get('tts', 0)
        ])
        
        # gpu_type ê°™ì€ ë¬¸ìì—´ ê°’ ì œì™¸í•˜ê³  ìˆ«ì ê°’ë§Œ ë¹„êµ
        active_services = [service for service, count in requirements.items() 
                          if isinstance(count, (int, float)) and count > 0]
        primary_services = [service for service, count in requirements.items() 
                           if isinstance(count, (int, float)) and count > 0 and service in ['callbot', 'advisor']]
        
        service_loads['infra_breakdown'] = {
            'total_channels': total_channels,
            'services': active_services,
            'primary_services': primary_services if primary_services else active_services[:2]  # ìƒìœ„ 2ê°œ ì„œë¹„ìŠ¤
        }
        
        return service_loads
    
    def _calculate_gpu_requirements(self, loads: Dict[str, float], gpu_type: str, total_channels: int = 0, requirements: Dict[str, int] = None) -> Dict[str, float]:
        """GPU ìš”êµ¬ì‚¬í•­ ê³„ì‚° (ì„œë¹„ìŠ¤ë³„ ì±„ë„ ê¸°ë°˜ ë™ì  ìŠ¤ì¼€ì¼ë§)"""
        
        capacity = self.gpu_capacity['gpu_capacity'][gpu_type]['processing_capacity']
        
        # ì „ì²´ ì±„ë„ ìˆ˜ ê¸°ë°˜ NLP ìŠ¤ì¼€ì¼ë§ ë°°ìˆ˜
        if total_channels <= 100:
            nlp_multiplier = 1.0
        elif total_channels <= 500:
            nlp_multiplier = 1.5
        else:
            nlp_multiplier = 2.5
        
        # AICM ì „ìš© ì±„ë„ ìˆ˜ ê³„ì‚° (TA, QA ì œì™¸ - AICM ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        aicm_channels = 0
        if requirements:
            aicm_channels = (requirements.get('callbot', 0) + 
                           requirements.get('chatbot', 0) + 
                           requirements.get('advisor', 0) + 
                           requirements.get('stt', 0) +  # ë…ë¦½ STTëŠ” AICM ë¯¸ì‚¬ìš©ì´ì§€ë§Œ ì†ŒëŸ‰
                           requirements.get('tts', 0))   # ë…ë¦½ TTSë„ AICM ë¯¸ì‚¬ìš©ì´ì§€ë§Œ ì†ŒëŸ‰
        
        # AICM ì „ìš© ìŠ¤ì¼€ì¼ë§ ë°°ìˆ˜
        if aicm_channels <= 100:
            aicm_multiplier = 1.0
        elif aicm_channels <= 500:
            aicm_multiplier = 1.5
        else:
            aicm_multiplier = 2.5
        
        # TTS GPU ê³„ì‚° (ìºì‹œ ìµœì í™” ì ìš©, ìŠ¤ì¼€ì¼ë§ ì œì™¸)
        tts_gpu_needed = math.ceil(loads['tts_channels'] / capacity['tts']['with_cache_optimization'])
        
        # NLP GPU ê³„ì‚° (9ì‹œê°„ ê·¼ë¬´ ê¸°ì¤€, ì „ì²´ ì±„ë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§)
        working_hours = 9  # 9ì‹œê°„ ê·¼ë¬´ ê¸°ì¤€ (ì ì‹¬ì‹œê°„ ì œì™¸ 8ì‹œê°„ + ì—¬ìœ  1ì‹œê°„)
        nlp_qps_needed = loads['nlp_daily_queries'] / (working_hours * 3600)
        nlp_gpu_base = math.ceil(nlp_qps_needed / capacity['nlp']['queries_per_second'])
        nlp_gpu_needed = max(1, math.ceil(nlp_gpu_base * nlp_multiplier))
        
        # AICM GPU ê³„ì‚° (9ì‹œê°„ ê·¼ë¬´ ê¸°ì¤€, AICM ì „ìš© ì±„ë„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§)
        aicm_qps_needed = loads['aicm_daily_queries'] / (working_hours * 3600)
        aicm_gpu_base = math.ceil(aicm_qps_needed / capacity['aicm']['vector_searches_per_second'])
        aicm_gpu_needed = max(1, math.ceil(aicm_gpu_base * aicm_multiplier))
        
        return {
            'tts': float(tts_gpu_needed),
            'nlp': float(nlp_gpu_needed),
            'aicm': float(aicm_gpu_needed),
            'total': float(tts_gpu_needed + nlp_gpu_needed + aicm_gpu_needed)
        }
    
    def _calculate_cpu_requirements(self, loads: Dict[str, float]) -> Dict[str, float]:
        """CPU ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
        
        capacity = self.gpu_capacity['cpu_capacity']
        
        # STT CPU ê³„ì‚°
        stt_cpu_needed = loads['stt_channels'] / capacity['stt']['channels_per_core']
        
        # TA CPU ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬ + ìœ íœ´ì‹œê°„ í™œìš©)
        # TA: ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìœ íœ´ì‹œê°„ í™œìš©, ì‹¤ì œ í•„ìš”ëŸ‰ì˜ 20% ì ìš©
        if loads['ta_daily_processing'] > 0:
            ta_sentences_per_minute = loads['ta_daily_processing'] * (50 / 3)  # ì±„ë„ë‹¹ ë¶„ë‹¹ 16.67ë¬¸ì¥
            ta_cores_needed = ta_sentences_per_minute / 150  # 1ì½”ì–´ë‹¹ 150ë¬¸ì¥/ë¶„
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì¸ìˆ˜ ì ìš© (correlation_matrix.jsonì—ì„œ ì •ì˜)
            batch_processing_factor = 0.3  # ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±
            idle_utilization_factor = 0.67  # ìœ íœ´ì‹œê°„ í™œìš©ìœ¼ë¡œ ì¶”ê°€ ì ˆê°
            
            # ìµœì¢… CPU ìš”êµ¬ëŸ‰: ì‹¤ì œ í•„ìš”ëŸ‰ì˜ ì•½ 20% (0.3 * 0.67 = 0.201)
            ta_cpu_needed = ta_cores_needed * batch_processing_factor * idle_utilization_factor
            ta_cpu_needed = max(0.5, ta_cpu_needed)  # ìµœì†Œ 0.5ì½”ì–´ ë³´ì¥ (ë°°ì¹˜ ì²˜ë¦¬ íŠ¹ì„± ë°˜ì˜)
        else:
            ta_cpu_needed = 0.0  # TA ì…ë ¥ì´ ì—†ìœ¼ë©´ 0ì½”ì–´
        
        # QA CPU ê³„ì‚° (ë¦´ë ˆì´ ì„œë²„ ê°œë…)
        # QA: 1ì±„ë„ë‹¹ 3ë¶„ë‹¹ 1ê°œ ì§ˆì˜, 20% ë™ì‹œ ë°œìƒë¥  ì ìš©
        qa_channels = loads['qa_daily_evaluations'] / 200  # ì¼ì¼ 200ê±´ í‰ê°€ = 1ì±„ë„
        concurrent_channels = qa_channels * 0.2  # 20% ë™ì‹œ ë°œìƒ
        qa_cpu_needed = concurrent_channels / 10.0  # 1ì½”ì–´ë‹¹ 10ê°œ ë™ì‹œ ì§ˆì˜ ì²˜ë¦¬
        
        # ì¸í”„ë¼ CPUëŠ” ë³„ë„ ê³„ì‚° í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬
        infra_cpu = 0  # ê¸°ë³¸ê°’, ì‹¤ì œëŠ” _calculate_infrastructure_requirementsì—ì„œ ê³„ì‚°
        
        return {
            'stt': stt_cpu_needed,
            'ta': ta_cpu_needed,
            'qa': qa_cpu_needed,
            'infrastructure': infra_cpu,  # ì„ì‹œê°’
            'total': stt_cpu_needed + ta_cpu_needed + qa_cpu_needed  # ì¸í”„ë¼ ì œì™¸
        }
    
    def _calculate_network_requirements(self, loads: Dict[str, float]) -> Dict[str, float]:
        """ë„¤íŠ¸ì›Œí¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
        
        correlation = self.correlation_matrix['correlation_weights']
        
        base_traffic = 0
        api_traffic = 0
        
        # ë©”ì¸ ì„œë¹„ìŠ¤ ê¸°ë³¸ íŠ¸ë˜í”½ (ì‹¤ì œ ì‚¬ìš©ëŸ‰ ê¸°ë°˜)
        base_traffic_weights = correlation['network_requirements']['base_traffic']
        
        # ì½œë´‡ íŠ¸ë˜í”½
        if 'callbot' in loads:
            base_traffic += loads['callbot'] * base_traffic_weights['callbot_mbps_per_channel']
        
        # ì±—ë´‡ íŠ¸ë˜í”½
        if 'chatbot' in loads:
            base_traffic += loads['chatbot'] * base_traffic_weights['chatbot_mbps_per_user']
        
        # ì–´ë“œë°”ì´ì € íŠ¸ë˜í”½
        if 'advisor' in loads:
            base_traffic += loads['advisor'] * base_traffic_weights['advisor_mbps_per_advisor']
        
        # API íŠ¸ë˜í”½ (ì¿¼ë¦¬ ê¸°ë°˜)
        api_weights = correlation['network_requirements']['api_traffic']
        nlp_traffic = (loads['nlp_daily_queries'] / (9 * 3600)) * api_weights['nlp_query_kb'] / 1024  # KB to MB
        aicm_traffic = (loads['aicm_daily_queries'] / (9 * 3600)) * api_weights['aicm_query_kb'] / 1024  # KB to MB
        api_traffic = nlp_traffic + aicm_traffic
        
        return {
            'base': base_traffic,
            'api': api_traffic,
            'total': base_traffic + api_traffic,
            'peak_factor': 1.5  # í”¼í¬ ì‹œê°„ 1.5ë°°
        }
    
    def _calculate_storage_requirements(self, loads: Dict[str, float]) -> Dict[str, float]:
        """ìŠ¤í† ë¦¬ì§€ ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
        
        storage_weights = self.correlation_matrix['correlation_weights']['storage_requirements']['daily_mb_per_unit']
        
        daily_mb = 0
        for service, mb_per_unit in storage_weights.items():
            if service in loads:
                daily_mb += loads[service] * mb_per_unit
        
        # ë…¹ì·¨ íŒŒì¼ ë³„ë„ ê³„ì‚°
        recording_mb = loads['stt_channels'] * 3.2  # STT ì±„ë„ë‹¹ ì¼ì¼ ë…¹ì·¨ëŸ‰
        
        return {
            'daily_mb': daily_mb + recording_mb,
            'monthly_gb': (daily_mb + recording_mb) * 30 / 1024,
            'yearly_tb': (daily_mb + recording_mb) * 365 / (1024 * 1024),
            'with_backup_tb': (daily_mb + recording_mb) * 365 * 2 / (1024 * 1024)  # ì´ì¤‘ ë°±ì—…
        }
    
    def _calculate_infrastructure_requirements(self, total_req: Dict[str, int]) -> Dict[str, float]:
        """ê³µí†µ ì¸í”„ë¼ ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
        
        infra_specs = self.infrastructure['infrastructure']['base_requirements']
        # gpu_type ê°™ì€ ë¬¸ìì—´ ê°’ ì œì™¸í•˜ê³  ìˆ«ì ê°’ë§Œ í•©ê³„ ê³„ì‚°
        total_load = sum(v for v in total_req.values() if isinstance(v, (int, float)))
        
        infrastructure = {}
        
        # STTë‚˜ TTSë§Œ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì²´í¬
        stt_only = total_req.get('stt', 0) > 0 and all(v == 0 for k, v in total_req.items() if k not in ['stt', 'tts'])
        tts_only = total_req.get('tts', 0) > 0 and all(v == 0 for k, v in total_req.items() if k not in ['stt', 'tts'])
        
        # STT/TTSë§Œ ë…ë¦½ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì¸í”„ë¼ ì—†ì´ ì²˜ë¦¬
        if stt_only or tts_only:
            # ìµœì†Œí•œì˜ ëª¨ë‹ˆí„°ë§ë§Œ
            infrastructure['monitoring'] = 1
            return infrastructure
        
        # ì„œë¹„ìŠ¤ë³„ ìµœì í™”ëœ ê°€ì¤‘ì¹˜ (10~5000ì±„ë„ ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘)
        service_weights = {
            'callbot': {'nginx': 0.014, 'gateway': 0.011, 'database': 0.018, 'auth_service': 0.004},
            'chatbot': {'nginx': 0.011, 'gateway': 0.008, 'database': 0.014, 'auth_service': 0.003},
            'advisor': {'nginx': 0.013, 'gateway': 0.010, 'database': 0.021, 'vector_db': 0.011, 'auth_service': 0.004},
            'ta': {'nginx': 0.006, 'gateway': 0.004, 'database': 0.011, 'auth_service': 0.001},
            'qa': {'nginx': 0.004, 'gateway': 0.003, 'database': 0.008, 'auth_service': 0.001},
            'stt': {'nginx': 0.006, 'gateway': 0.005, 'database': 0.008, 'auth_service': 0.001},
            'tts': {'nginx': 0.006, 'gateway': 0.005, 'database': 0.008, 'auth_service': 0.001}
        }
        
        # ê° ì¸í”„ë¼ ì„œë¹„ìŠ¤ë³„ ê°€ì¤‘ ë¶€í•˜ ê³„ì‚°
        weighted_loads = {}
        for service, count in total_req.items():
            if count > 0 and service in service_weights:
                for infra_service, weight in service_weights[service].items():
                    weighted_loads[infra_service] = weighted_loads.get(infra_service, 0) + (count * weight)
        
        # ì¸í”„ë¼ ì„œë²„ ì‚¬ì–‘ ê³„ì‚°
        if weighted_loads:
            for service, config in infra_specs.items():
                base_cpu = config.get('cpu_cores', 0)
                load = weighted_loads.get(service, 0)
                infrastructure[service] = base_cpu + load
        else:
            # ì„œë¹„ìŠ¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¸í”„ë¼ë§Œ
            for service, config in infra_specs.items():
                infrastructure[service] = config.get('base_cpu', 0)
        
        return infrastructure
    
    def _calculate_dynamic_gpu_ram(self, base_ram: int, workload_intensity: float, max_ram: int = 80) -> int:
        """GPU ì„œë²„ì˜ RAMì„ ì›Œí¬ë¡œë“œ ê°•ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            base_ram: GPU ê¸°ë³¸ RAM (16GB, 32GB, 48GB ë“±)
            workload_intensity: ì›Œí¬ë¡œë“œ ê°•ë„ (0.1 ~ 1.0)
            max_ram: ìµœëŒ€ RAM ì œí•œ (ê¸°ë³¸ê°’: 80GB)
            
        Returns:
            ê³„ì‚°ëœ RAM (GB)
        """
        # ê¸°ë³¸ RAMì˜ 1.25ë°° ~ ìµœëŒ€ RAM ì‚¬ì´ì—ì„œ ì›Œí¬ë¡œë“œ ê°•ë„ì— ë”°ë¼ ì¡°ì • (ê°€ì¤‘ì¹˜ ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ)
        min_multiplier = 1.25  # ìµœì†Œ 1.25ë°° (ê¸°ì¡´ 1.5ë°°ì—ì„œ ì¶•ì†Œ)
        max_multiplier = max_ram / base_ram  # ìµœëŒ€ ë°°ìˆ˜ëŠ” max_ram ê¸°ì¤€
        
        # ì›Œí¬ë¡œë“œ ê°•ë„ì— ë”°ë¥¸ ë°°ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ë²”ìœ„ ì¶•ì†Œ)
        effective_intensity = workload_intensity * 0.5  # ê°€ì¤‘ì¹˜ ì˜í–¥ë„ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¶•ì†Œ
        multiplier = min_multiplier + (max_multiplier - min_multiplier) * effective_intensity
        
        # ê³„ì‚°ëœ RAM
        calculated_ram = int(base_ram * multiplier)
        
        # ìµœëŒ€ê°’ ì œí•œ ì ìš©
        final_ram = min(calculated_ram, max_ram)
        
        # ë””ë²„ê·¸ ë¡œê¹…
        logging.info(f"ğŸ”§ ë™ì  GPU RAM ê³„ì‚°: base_ram={base_ram}GB, workload_intensity={workload_intensity:.2f}, multiplier={multiplier:.2f}, calculated_ram={calculated_ram}GB, final_ram={final_ram}GB")
        
        return final_ram

    def _calculate_actual_memory_requirements(self, hardware_spec: 'HardwareSpecification') -> float:
        """ì‹¤ì œ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""
        total_memory = 0.0
        
        # GPU ì„œë²„ ë©”ëª¨ë¦¬
        for server in hardware_spec.gpu_servers:
            total_memory += server['ram_gb'] * server['count']
        
        # CPU ì„œë²„ ë©”ëª¨ë¦¬
        for server in hardware_spec.cpu_servers:
            total_memory += server['ram_gb'] * server['count']
        
        # ì¸í”„ë¼ ì„œë²„ ë©”ëª¨ë¦¬
        for server in hardware_spec.infrastructure_servers:
            total_memory += server['ram_gb'] * server['count']
        
        return total_memory

    def _generate_hardware_specification(self, resources: ResourceCalculation, requirements: Dict[str, int], total_channels: int = 0) -> HardwareSpecification:
        """í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ìƒì„±"""
        servers = []
        
        # GPU ì„œë²„ ìƒì„± (TTS, NLP, AICM)
        if resources.gpu['tts'] > 0:
            gpu_per_server = 1
            tts_servers_needed = max(1, math.ceil(resources.gpu['tts'] / gpu_per_server))
            
            # GPU íƒ€ì…ë³„ ì‚¬ì–‘ ì ìš©
            gpu_type = self._select_gpu_type(resources.gpu['tts'], total_channels)
            gpu_specs = self.gpu_capacity['gpu_capacity'][gpu_type.lower()]['specs']
            
            cpu_cores = gpu_specs['cpu_cores']
            
            # TTS ì›Œí¬ë¡œë“œ ê°•ë„ ê³„ì‚° (ì±„ë„ ìˆ˜ ê¸°ë°˜)
            tts_workload_intensity = min(1.0, resources.tts_channels / 100.0)  # 100ì±„ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            
            # ë™ì  RAM ê³„ì‚° (GPU ê¸°ë³¸ RAM ê¸°ì¤€)
            base_gpu_ram = gpu_specs['ram_gb']
            ram_gb = self._calculate_dynamic_gpu_ram(base_gpu_ram, tts_workload_intensity)
            
            servers.append({
                'type': f'TTS ì„œë²„ ({gpu_type})',
                'count': tts_servers_needed,
                'cpu_cores': cpu_cores,
                'ram_gb': ram_gb,
                'storage_nvme_tb': 0.5,  # 500GB ê³ ì •
                'gpu_per_server': gpu_per_server,
                'purpose': f'TTS ìŒì„± í•©ì„± (ì½œë´‡ ì „ìš©, {resources.tts_channels}ì±„ë„)',
                'gpu_allocation': {
                    'tts': f"{resources.gpu['tts']:.1f}",
                    'nlp': '0.0',
                    'aicm': '0.0'
                }
            })
        
        if resources.gpu['nlp'] > 0:
            gpu_per_server = 1
            nlp_servers_needed = max(1, math.ceil(resources.gpu['nlp'] / gpu_per_server))
            
            # GPU íƒ€ì…ë³„ ê¸°ë³¸ ì‚¬ì–‘
            gpu_type = self._select_gpu_type(resources.gpu['nlp'], total_channels)
            gpu_specs = self.gpu_capacity['gpu_capacity'][gpu_type.lower()]['specs']
            
            # ì„œë²„ ìŠ¤í™ ì„¤ì •
            fixed_cpu = 32  # ê³ ì • 32ì½”ì–´
            gpu_count = resources.gpu['nlp']
            
            # NLP ì›Œí¬ë¡œë“œ ê°•ë„ ê³„ì‚° (ì´ ì¿¼ë¦¬ ìˆ˜ ê¸°ë°˜)
            total_nlp_queries = sum(resources.nlp_breakdown.values()) if resources.nlp_breakdown else 0
            nlp_workload_intensity = min(1.0, total_nlp_queries / 500000.0)  # 50ë§Œ ì¿¼ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            
            logging.info(f"ğŸ“Š NLP ì›Œí¬ë¡œë“œ ë¶„ì„: total_queries={total_nlp_queries}, intensity={nlp_workload_intensity:.2f}")
            
            # ë™ì  RAM ê³„ì‚° (GPU ê¸°ë³¸ RAM ê¸°ì¤€)
            base_gpu_ram = gpu_specs['ram_gb']
            dynamic_ram = self._calculate_dynamic_gpu_ram(base_gpu_ram, nlp_workload_intensity)
            
            # ìŠ¤í† ë¦¬ì§€ë§Œ GPU ìˆ˜ì— ë”°ë¼ ì¡°ì •
            # scaled_storage = max(4.0, gpu_count * 1.0)  # GPUë‹¹ 1TBì”© ì¦ê°€, ìµœì†Œ 4TB
            
            # NLP ì¢…ì†ì„± ì •ë³´ ìƒì„±
            nlp_dependency = self._format_nlp_dependency(resources.nlp_breakdown)
            
            servers.append({
                'type': f'NLP ì„œë²„ ({gpu_type})',
                'count': nlp_servers_needed,
                'cpu_cores': fixed_cpu,
                'ram_gb': dynamic_ram,
                'storage_nvme_tb': 0.5,  # 500GB ê³ ì •
                'gpu_per_server': gpu_per_server,
                'purpose': f'NLP ìì—°ì–´ ì²˜ë¦¬ ({nlp_dependency})',
                'gpu_allocation': {
                    'tts': '0.0',
                    'nlp': f"{resources.gpu['nlp']:.1f}",
                    'aicm': '0.0'
                }
            })
        
        if resources.gpu['aicm'] > 0:
            gpu_per_server = 1
            aicm_servers_needed = max(1, math.ceil(resources.gpu['aicm'] / gpu_per_server))
            
            # AICM ì „ìš© ì±„ë„ ìˆ˜ ê³„ì‚° (GPU íƒ€ì… ì„ íƒìš©)
            aicm_channels = (requirements.get('callbot', 0) + 
                           requirements.get('chatbot', 0) + 
                           requirements.get('advisor', 0))
            
            # GPU íƒ€ì…ë³„ ê¸°ë³¸ ì‚¬ì–‘ (AICM ì „ìš© ì±„ë„ ê¸°ë°˜)
            gpu_type = self._select_gpu_type(resources.gpu['aicm'], aicm_channels)
            gpu_specs = self.gpu_capacity['gpu_capacity'][gpu_type.lower()]['specs']
            
            # ì„œë²„ ìŠ¤í™ ì„¤ì •
            fixed_cpu = 32  # ê³ ì • 32ì½”ì–´
            gpu_count = resources.gpu['aicm']
            
            # AICM ì›Œí¬ë¡œë“œ ê°•ë„ ê³„ì‚° (ì´ ì¿¼ë¦¬ ìˆ˜ ê¸°ë°˜)
            total_aicm_queries = sum(resources.aicm_breakdown.values()) if resources.aicm_breakdown else 0
            aicm_workload_intensity = min(1.0, total_aicm_queries / 300000.0)  # 30ë§Œ ì¿¼ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            
            logging.info(f"ğŸ“Š AICM ì›Œí¬ë¡œë“œ ë¶„ì„: total_queries={total_aicm_queries}, intensity={aicm_workload_intensity:.2f}")
            
            # ë™ì  RAM ê³„ì‚° (GPU ê¸°ë³¸ RAM ê¸°ì¤€)
            base_gpu_ram = gpu_specs['ram_gb']
            dynamic_ram = self._calculate_dynamic_gpu_ram(base_gpu_ram, aicm_workload_intensity)
            
            # ìŠ¤í† ë¦¬ì§€ë§Œ GPU ìˆ˜ì— ë”°ë¼ ì¡°ì •
            # scaled_storage = max(4.0, gpu_count * 1.0)  # GPUë‹¹ 1TBì”© ì¦ê°€, ìµœì†Œ 4TB
            
            # AICM ì¢…ì†ì„± ì •ë³´ ìƒì„±
            aicm_dependency = self._format_aicm_dependency(resources.aicm_breakdown)
            
            servers.append({
                'type': f'AICM ì„œë²„ ({gpu_type})',
                'count': aicm_servers_needed,
                'cpu_cores': fixed_cpu,
                'ram_gb': dynamic_ram,
                'storage_nvme_tb': 0.5,  # 500GB ê³ ì •
                'gpu_per_server': gpu_per_server,
                'purpose': f'AICM ë²¡í„° ê²€ìƒ‰ RAG ({aicm_dependency})',
                'gpu_allocation': {
                    'tts': '0.0',
                    'nlp': '0.0',
                    'aicm': f"{resources.gpu['aicm']:.1f}"
                }
            })
        
        # CPU ì„œë²„ ìƒì„± (STT, TA, QA)
        if resources.cpu['stt'] > 0:
            stt_instances = self._find_optimal_instance_combination(resources.cpu['stt'], 0)  # ìŠ¤í† ë¦¬ì§€ëŠ” ë³„ë„ ê³„ì‚°
            total_cores = sum(instance['cpu_cores'] * instance['count'] for instance in stt_instances)
            
            # STT ì¢…ì†ì„± ì •ë³´ ìƒì„±
            stt_dependency = self._format_stt_dependency(resources.stt_breakdown)
            
            for instance in stt_instances:
                # ê° ì„œë²„ê°€ ì²˜ë¦¬í•˜ëŠ” ì±„ë„ ë¹„ìœ¨ì— ë”°ë¥¸ ìŠ¤í† ë¦¬ì§€ ê³„ì‚°
                server_cores = instance['cpu_cores'] * instance['count']
                server_channels = resources.stt_channels * (server_cores / total_cores)
                server_storage = max(0.5, round(server_channels * 0.01, 1))  # ì±„ë„ë‹¹ 10GB, ìµœì†Œ 0.5TB, ì†Œìˆ˜ì  1ìë¦¬
                
                servers.append({
                    'type': f'STT ì„œë²„ ({instance["cpu_cores"]}ì½”ì–´)',
                    'count': instance['count'],
                    'cpu_cores': instance['cpu_cores'],
                    'ram_gb': instance['ram_gb'],
                    'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                    'purpose': f'ìŒì„± ì¸ì‹ ì „ìš© (1ì½”ì–´ë‹¹ 6.5ì±„ë„, ì´ {server_cores}ì½”ì–´) - {stt_dependency}',
                    'cpu_allocation': {
                        'stt': f"{resources.cpu['stt']:.1f}",
                        'ta': '0.0',
                        'qa': '0.0',
                        'infrastructure': '0.0'
                    }
                })
        
        # TA CPU ì„œë²„ ìƒì„± (TA ì…ë ¥ì´ ìˆì„ ë•Œë§Œ) - í†µí•© ë°©ì‹
        if resources.cpu['ta'] > 0:
            ta_instances = self._find_optimal_instance_combination(resources.cpu['ta'], resources.ta_channels)
            
            # TA ì„œë²„ í†µí•© ì²˜ë¦¬ (STTì™€ ë™ì¼í•œ ë°©ì‹)
            if ta_instances:
                # ì´ ì½”ì–´ ìˆ˜ì™€ ìŠ¤í† ë¦¬ì§€ ê³„ì‚°
                total_ta_cores = sum(instance['cpu_cores'] * instance['count'] for instance in ta_instances)
                total_ta_storage = sum(instance['storage_ssd_tb'] * instance['count'] for instance in ta_instances)
                total_ta_count = sum(instance['count'] for instance in ta_instances)
                
                # ëŒ€í‘œ ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ (ê°€ì¥ í° ì¸ìŠ¤í„´ìŠ¤)
                main_instance = max(ta_instances, key=lambda x: x['cpu_cores'])
                
                servers.append({
                    'type': f'TA CPU ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                    'count': total_ta_count,
                    'cpu_cores': main_instance['cpu_cores'],
                    'ram_gb': main_instance['ram_gb'],
                    'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                    'purpose': f'TA {int(resources.ta_channels)}ì±„ë„ ë¶„ì„ ì²˜ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬, NLP ì—°ë™)',
                    'cpu_allocation': {
                        'stt': '0.0',
                        'ta': f"{resources.cpu['ta']:.1f}",
                        'qa': '0.0',
                        'infrastructure': '0.0'
                    }
                })
        
        if resources.cpu['qa'] > 0:
            # QA ì±„ë„ ìˆ˜ë¥¼ requirementsì—ì„œ ì§ì ‘ ê³„ì‚°
            qa_channels = requirements.get('qa', 0)
            qa_instances = self._find_optimal_instance_combination(resources.cpu['qa'], qa_channels)
            for instance in qa_instances:
                # QA ì„œë²„ ìŠ¤í† ë¦¬ì§€: ì±„ë„ë‹¹ 10GB
                qa_storage = max(1.0, qa_channels * 0.01)  # ì±„ë„ë‹¹ 10GB, ìµœì†Œ 1TB
                servers.append({
                    'type': f'QA ì„œë²„ ({instance["cpu_cores"]}ì½”ì–´)',
                    'count': instance['count'],
                    'cpu_cores': instance['cpu_cores'],
                    'ram_gb': instance['ram_gb'],
                    'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                    'purpose': f'QA {qa_channels}ì±„ë„ í’ˆì§ˆ í‰ê°€ (ì™¸ë¶€ LLM ê¸°ë°˜, ë°°ì¹˜ ì²˜ë¦¬)',
                    'cpu_allocation': {
                        'stt': '0.0',
                        'ta': '0.0',
                        'qa': f"{resources.cpu['qa']:.1f}",
                        'infrastructure': '0.0'
                    }
                })
        
        # ì¸í”„ë¼ ì„œë²„ ìƒì„±
        total_users = sum([
            requirements.get('callbot', 0),
            requirements.get('chatbot', 0), 
            requirements.get('advisor', 0),
            requirements.get('ta', 0),
            requirements.get('qa', 0)
        ])
        
        # Nginx ì„œë²„ (500GB ê³ ì •, STT/TAì²˜ëŸ¼ í†µí•© ë°©ì‹)
        nginx_cpu_total = max(2, int(resources.infrastructure.get('nginx', 2)))  # ìµœì†Œ 2ì½”ì–´
        nginx_instances = self._find_optimal_instance_combination(nginx_cpu_total)
        
        if nginx_instances:
            total_nginx_cores = sum(instance['cpu_cores'] * instance['count'] for instance in nginx_instances)
            total_nginx_count = sum(instance['count'] for instance in nginx_instances)
            
            main_instance = max(nginx_instances, key=lambda x: x['cpu_cores'])
            
            servers.append({
                'type': f'Nginx ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                'count': total_nginx_count,
                'cpu_cores': main_instance['cpu_cores'],
                'ram_gb': main_instance['cpu_cores'] * 2,  # ì½”ì–´ë‹¹ 2GB RAM
                'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                'purpose': f'{self._format_infra_dependency(resources.infra_breakdown, "nginx")}',
                'cpu_allocation': {
                    'stt': '0.0',
                    'ta': '0.0',
                    'qa': '0.0',
                    'infrastructure': f'{nginx_cpu_total}.0'
                }
            })
        
        # API Gateway ì„œë²„ (500GB ê³ ì •, STT/TAì²˜ëŸ¼ í†µí•© ë°©ì‹, ê¸°ë³¸ 2ëŒ€ êµ¬ì„±)
        gateway_cpu_total = max(2, int(resources.infrastructure.get('gateway', 2)))  # ìµœì†Œ 2ì½”ì–´
        gateway_instances = self._find_optimal_instance_combination(gateway_cpu_total)
        
        if gateway_instances:
            total_gateway_cores = sum(instance['cpu_cores'] * instance['count'] for instance in gateway_instances)
            total_gateway_count = sum(instance['count'] for instance in gateway_instances) * 2  # ê¸°ë³¸ 2ëŒ€ì”© êµ¬ì„±
            
            main_instance = max(gateway_instances, key=lambda x: x['cpu_cores'])
            
            servers.append({
                'type': f'API Gateway ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                'count': total_gateway_count,
                'cpu_cores': main_instance['cpu_cores'],
                'ram_gb': main_instance['cpu_cores'] * 2,  # ì½”ì–´ë‹¹ 2GB RAM
                'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                'purpose': f'{self._format_infra_dependency(resources.infra_breakdown, "gateway")} (ì´ {total_gateway_cores * 2}ì½”ì–´, ì´ì¤‘í™”)',
                'cpu_allocation': {
                    'stt': '0.0',
                    'ta': '0.0',
                    'qa': '0.0',
                    'infrastructure': f'{gateway_cpu_total * 2}.0'  # 2ëŒ€ì´ë¯€ë¡œ 2ë°°
                }
            })
        
        # PostgreSQL ì„œë²„ (ì‚¬ìš©ìë‹¹ 5GBì”© ì¦ê°€, STT/TAì²˜ëŸ¼ í†µí•© ë°©ì‹)
        postgres_storage = max(1.0, total_users * 0.005)  # ì‚¬ìš©ìë‹¹ 5GB, ìµœì†Œ 1TB
        postgres_cpu_total = max(4, int(resources.infrastructure.get('database', 4)))  # ìµœì†Œ 4ì½”ì–´
        postgres_instances = self._find_optimal_instance_combination(postgres_cpu_total)
        
        if postgres_instances:
            total_postgres_cores = sum(instance['cpu_cores'] * instance['count'] for instance in postgres_instances)
            total_postgres_count = sum(instance['count'] for instance in postgres_instances)
            
            main_instance = max(postgres_instances, key=lambda x: x['cpu_cores'])
            
            servers.append({
                'type': f'PostgreSQL ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                'count': total_postgres_count,
                'cpu_cores': main_instance['cpu_cores'],
                'ram_gb': main_instance['cpu_cores'] * 4,  # ì½”ì–´ë‹¹ 4GB RAM
                'storage_ssd_tb': round(postgres_storage, 1),
                'purpose': f'{self._format_infra_dependency(resources.infra_breakdown, "database")}',
                'cpu_allocation': {
                    'stt': '0.0',
                    'ta': '0.0',
                    'qa': '0.0',
                    'infrastructure': f'{postgres_cpu_total}.0'
                }
            })
        
        # VectorDB ì„œë²„ (4TB ê³ ì •, STT/TAì²˜ëŸ¼ í†µí•© ë°©ì‹) - ì–´ë“œë°”ì´ì € ì‚¬ìš© ì‹œë§Œ
        if resources.infrastructure.get('vector_db', 0) > 0:
            vectordb_cpu_total = max(8, int(resources.infrastructure.get('vector_db', 8)))  # ìµœì†Œ 8ì½”ì–´
            vectordb_instances = self._find_optimal_instance_combination(vectordb_cpu_total)
            
            if vectordb_instances:
                total_vectordb_cores = sum(instance['cpu_cores'] * instance['count'] for instance in vectordb_instances)
                total_vectordb_storage = sum(instance.get('storage_ssd_tb', 0.5) * instance['count'] for instance in vectordb_instances)
                total_vectordb_count = sum(instance['count'] for instance in vectordb_instances)
                
                main_instance = max(vectordb_instances, key=lambda x: x['cpu_cores'])
                
                servers.append({
                    'type': f'VectorDB ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                    'count': total_vectordb_count,
                    'cpu_cores': main_instance['cpu_cores'],
                    'ram_gb': main_instance['cpu_cores'] * 4,  # ì½”ì–´ë‹¹ 4GB RAM
                    'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                    'purpose': f'{self._format_infra_dependency(resources.infra_breakdown, "vector_db")} (ì´ {total_vectordb_cores}ì½”ì–´)',
                    'cpu_allocation': {
                        'stt': '0.0',
                        'ta': '0.0',
                        'qa': '0.0',
                        'infrastructure': f'{vectordb_cpu_total}.0'
                    }
                })
        
        # Auth Service ì„œë²„ (500GB ê³ ì •, STT/TAì²˜ëŸ¼ í†µí•© ë°©ì‹)
        auth_cpu_total = max(4, int(resources.infrastructure.get('auth_service', 4)))  # ìµœì†Œ 4ì½”ì–´
        auth_instances = self._find_optimal_instance_combination(auth_cpu_total)
        
        if auth_instances:
            total_auth_cores = sum(instance['cpu_cores'] * instance['count'] for instance in auth_instances)
            total_auth_count = sum(instance['count'] for instance in auth_instances)
            
            main_instance = max(auth_instances, key=lambda x: x['cpu_cores'])
            
            servers.append({
                'type': f'Auth Service ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                'count': total_auth_count,
                'cpu_cores': main_instance['cpu_cores'],
                'ram_gb': main_instance['cpu_cores'] * 2,  # ì½”ì–´ë‹¹ 2GB RAM
                'storage_ssd_tb': 0.5,  # 500GB ê³ ì •
                'purpose': f'{self._format_infra_dependency(resources.infra_breakdown, "auth_service")} (ì´ {total_auth_cores}ì½”ì–´)',
                'cpu_allocation': {
                    'stt': '0.0',
                    'ta': '0.0',
                    'qa': '0.0',
                    'infrastructure': f'{auth_cpu_total}.0'
                }
            })
        
        # NAS ì„œë²„ (ìœ ì € ê¸°ë°˜ ë™ì  ìŠ¤í† ë¦¬ì§€, ìµœì†Œ 1TB)
        total_users = sum(requirements.values()) if requirements else 200  # ê¸°ë³¸ 200 ìœ ì €
        nas_storage = max(1.0, total_users * 0.005)  # ìœ ì €ë‹¹ 5GB, ìµœì†Œ 1TB
        nas_cpu_total = max(8, int(nas_storage / 2))  # ìŠ¤í† ë¦¬ì§€ ê¸°ë°˜ CPU í• ë‹¹, ìµœì†Œ 8ì½”ì–´
        nas_instances = self._find_optimal_instance_combination(nas_cpu_total)
        
        if nas_instances:
            total_nas_cores = sum(instance['cpu_cores'] * instance['count'] for instance in nas_instances)
            total_nas_count = sum(instance['count'] for instance in nas_instances)
            
            main_instance = max(nas_instances, key=lambda x: x['cpu_cores'])
            
            servers.append({
                'type': f'NAS ì„œë²„ ({main_instance["cpu_cores"]}ì½”ì–´)',
                'count': total_nas_count,
                'cpu_cores': main_instance['cpu_cores'],
                'ram_gb': main_instance['cpu_cores'] * 2,  # ì½”ì–´ë‹¹ 2GB RAM
                'storage_ssd_tb': nas_storage,
                'purpose': f'ë„¤íŠ¸ì›Œí¬ ìŠ¤í† ë¦¬ì§€ (ì´ ìœ ì € {total_users}ëª…, {nas_storage:.1f}TB) (ì´ {total_nas_cores}ì½”ì–´)',
                'cpu_allocation': {
                    'stt': '0.0',
                    'ta': '0.0',
                    'qa': '0.0',
                    'infrastructure': f'{nas_cpu_total}.0'
                }
            })
        
        # ë„¤íŠ¸ì›Œí¬ ìš”êµ¬ì‚¬í•­
        total_channels = resources.stt_channels + resources.tts_channels + resources.ta_channels + resources.qa_channels
        if total_channels <= 100:
            network_req = "1 Gbps"
        elif total_channels <= 500:
            network_req = "5 Gbps"
        else:
            network_req = "10 Gbps"
        
        # ì¤‘ë³µ ì„œë²„ ì œê±°ëŠ” ì´ë¯¸ ìœ„ì—ì„œ í†µí•© ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬ë¨
        
        return HardwareSpecification(
            gpu_servers=[s for s in servers if 'gpu_allocation' in s],
            cpu_servers=[s for s in servers if 'cpu_allocation' in s and 'gpu_allocation' not in s and s.get('cpu_allocation', {}).get('infrastructure', '0.0') == '0.0'],
            storage_servers=[s for s in servers if 'storage_raid10_tb' in s],
            infrastructure_servers=[s for s in servers if 'cpu_allocation' in s and s.get('cpu_allocation', {}).get('infrastructure', '0.0') != '0.0'],
            network_requirements=network_req,
            infrastructure_notes="ì´ì¤‘í™” êµ¬ì„± ê¶Œì¥, ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í•„ìˆ˜"
        )
    
    def compare_gpu_options(self, requirements: Dict[str, int]) -> Dict[str, Any]:
        """GPU ì˜µì…˜ë³„ ë¹„êµ ë¶„ì„"""
        
        comparisons = {}
        for gpu_type in ['t4', 'v100', 'l40s']:
            resources, hardware = self.calculate_hardware_requirements(requirements, gpu_type)
            comparisons[gpu_type] = {
                'gpu_count': math.ceil(resources.gpu['total']),
                'server_count': sum(s['count'] for s in hardware.gpu_servers),
                'efficiency_score': self._calculate_efficiency_score(resources, gpu_type),
                'complexity_score': self._calculate_complexity_score(hardware)
            }
        
        # ê¶Œì¥ì‚¬í•­ ê²°ì •
        t4_gpus = comparisons['t4']['gpu_count']
        recommendation = 't4'
        if t4_gpus > 15:
            recommendation = 'l40s'
        elif t4_gpus > 5:
            recommendation = 'v100'
            
        comparisons['recommendation'] = recommendation
        return comparisons
    
    def _calculate_efficiency_score(self, resources: ResourceCalculation, gpu_type: str) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        gpu_specs = self.gpu_capacity['gpu_capacity'][gpu_type]['specs']
        power_efficiency = 100 / gpu_specs['power_watts']  # ì „ë ¥ íš¨ìœ¨
        memory_efficiency = gpu_specs['memory_gb'] / 16  # ë©”ëª¨ë¦¬ íš¨ìœ¨ (T4 ê¸°ì¤€)
        return (power_efficiency + memory_efficiency) / 2
    
    def _calculate_complexity_score(self, hardware: HardwareSpecification) -> int:
        """ê´€ë¦¬ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        total_servers = sum(s['count'] for s in hardware.gpu_servers + hardware.cpu_servers + hardware.storage_servers + hardware.infrastructure_servers)
        return total_servers

    def generate_server_config_table(self, hardware_spec):
        """ì„œë²„ êµ¬ì„± ìƒì„¸ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"DEBUG: generate_server_config_table ì‹œì‘")
            logger.info(f"DEBUG: hardware_spec íƒ€ì…: {type(hardware_spec)}")
            logger.info(f"DEBUG: hardware_spec êµ¬ì¡° - GPU: {len(hardware_spec.gpu_servers)}, CPU: {len(hardware_spec.cpu_servers)}, Storage: {len(hardware_spec.storage_servers)}, Infra: {len(hardware_spec.infrastructure_servers)}")
            
            # ì‹¤ì œ í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            table_rows = []
            
            # GPU ì„œë²„ êµ¬ì„± (TTS, NLP, AICM)
            for server in hardware_spec.gpu_servers:
                logger.info(f"DEBUG: GPU ì„œë²„ ì²˜ë¦¬: {server}")
                if server.get('count', 0) > 0:
                    # GPU íƒ€ì… ìë™ ê°ì§€
                    gpu_type = 'T4'
                    if 'V100' in str(server.get('type', '')):
                        gpu_type = 'V100'
                    elif 'L40S' in str(server.get('type', '')):
                        gpu_type = 'L40S'
                    
                    # GPU RAM ìë™ ê°ì§€
                    gpu_ram = 16  # ê¸°ë³¸ê°’
                    if gpu_type == 'V100':
                        gpu_ram = 32
                    elif gpu_type == 'L40S':
                        gpu_ram = 48
                    
                    table_rows.append({
                        'role': f"{server.get('type', 'GPU ì„œë²„')}",
                        'cpu_cores': server.get('cpu_cores', 0),
                        'ram_gb': server.get('ram_gb', 0),
                        'quantity': server.get('count', 0),
                        'ebs_gb': '-',
                        'instance_storage_gb': round(server.get('storage_nvme_tb', 0) * 1000) if server.get('storage_nvme_tb') else '-',
                        'nas': '-',
                        'gpu_type': gpu_type,
                        'gpu_ram_gb': gpu_ram,
                        'gpu_quantity': server.get('gpu_per_server', 1)
                    })
            
            # CPU ì„œë²„ êµ¬ì„±
            for server in hardware_spec.cpu_servers:
                logger.info(f"DEBUG: CPU ì„œë²„ ì²˜ë¦¬: {server}")
                if server.get('count', 0) > 0:
                    table_rows.append({
                        'role': f"{server.get('type', 'CPU ì„œë²„')}",
                        'cpu_cores': server.get('cpu_cores', 0),
                        'ram_gb': server.get('ram_gb', 0),
                        'quantity': server.get('count', 0),
                        'ebs_gb': '-',
                        'instance_storage_gb': round(server.get('storage_ssd_tb', 0) * 1000) if server.get('storage_ssd_tb') else '-',
                        'nas': '-',
                        'gpu_type': '-',
                        'gpu_ram_gb': '-',
                        'gpu_quantity': '-'
                    })
            
            # ìŠ¤í† ë¦¬ì§€ ì„œë²„ êµ¬ì„±
            for server in hardware_spec.storage_servers:
                logger.info(f"DEBUG: ìŠ¤í† ë¦¬ì§€ ì„œë²„ ì²˜ë¦¬: {server}")
                if server.get('count', 0) > 0:
                    table_rows.append({
                        'role': f"{server.get('type', 'ìŠ¤í† ë¦¬ì§€ ì„œë²„')}",
                        'cpu_cores': server.get('cpu_cores', 0),
                        'ram_gb': server.get('ram_gb', 0),
                        'quantity': server.get('count', 0),
                        'ebs_gb': '-',
                        'instance_storage_gb': f"{server.get('storage_raid10_tb', 0)} TB",
                        'nas': '-',
                        'gpu_type': '-',
                        'gpu_ram_gb': '-',
                        'gpu_quantity': '-'
                    })
            
            # ì¸í”„ë¼ ì„œë²„ êµ¬ì„± (ê°ê° ê°œë³„ì ìœ¼ë¡œ í‘œì‹œ)
            for server in hardware_spec.infrastructure_servers:
                logger.info(f"DEBUG: ì¸í”„ë¼ ì„œë²„ ì²˜ë¦¬: {server}")
                if server.get('count', 0) > 0:
                    # ìŠ¤í† ë¦¬ì§€ ê°’ì´ 0ì´ë©´ '-'ë¡œ í‘œì‹œ
                    storage_value = server.get('storage_ssd_tb', 0)
                    storage_display = f"{storage_value} TB (SSD)" if storage_value > 0 else "-"
                    
                    table_rows.append({
                        'role': f"{server.get('type', 'ì¸í”„ë¼ ì„œë²„')}",
                        'cpu_cores': server.get('cpu_cores', 0),
                        'ram_gb': server.get('ram_gb', 0),
                        'quantity': server.get('count', 0),
                        'ebs_gb': '-',
                        'instance_storage_gb': round(storage_value * 1000) if storage_value > 0 else '-',
                        'nas': '-',
                        'gpu_type': '-',
                        'gpu_ram_gb': '-',
                        'gpu_quantity': '-'
                    })
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
            if not table_rows:
                logger.info("DEBUG: ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìŒ")
                return []
            
            logger.info(f"DEBUG: ìµœì¢… í…Œì´ë¸” í–‰ ìˆ˜: {len(table_rows)}")
            logger.info(f"DEBUG: í…Œì´ë¸” ë°ì´í„°: {table_rows}")
            return table_rows
            
        except Exception as e:
            logger.error(f"ì„œë²„ êµ¬ì„± í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ ë°˜í™˜
            return []

    def _select_gpu_type(self, gpu_count: float, total_channels: int = 0) -> str:
        """GPU ìˆ˜ëŸ‰ê³¼ ì±„ë„ ìˆ˜ì— ë”°ë¥¸ ì ì ˆí•œ GPU íƒ€ì… ì„ íƒ"""
        # ì†Œê·œëª¨ëŠ” ì±„ë„ ìˆ˜ ìš°ì„  ê³ ë ¤
        if total_channels <= 100:
            return 'T4'  # ì†Œê·œëª¨: ë¬´ì¡°ê±´ T4
        elif gpu_count <= 2:
            return 'T4'  # ì†ŒëŸ‰: T4
        elif gpu_count <= 8:
            return 'V100'  # ì¤‘ëŸ‰: V100
        else:
            return 'L40S'  # ëŒ€ëŸ‰: L40S
    
    def _format_stt_dependency(self, stt_breakdown: Dict[str, int]) -> str:
        """STT ì±„ë„ ì¢…ì†ì„± ì •ë³´ë¥¼ í¬ë§·íŒ…"""
        deps = []
        if stt_breakdown.get('callbot', 0) > 0:
            deps.append(f'ì½œë´‡ {stt_breakdown["callbot"]}ì±„ë„')
        # ì±—ë´‡ì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ë¯€ë¡œ STT ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        if stt_breakdown.get('advisor', 0) > 0:
            # ì–´ë“œë°”ì´ì €ëŠ” 1ëª…ë‹¹ 2ì±„ë„ì´ë¯€ë¡œ ëª…í™•íˆ í‘œì‹œ
            advisor_count = stt_breakdown["advisor"] // 2  # ì‹¤ì œ ì–´ë“œë°”ì´ì € ìˆ˜
            deps.append(f'ì–´ë“œë°”ì´ì € {stt_breakdown["advisor"]}ì±„ë„({advisor_count}ëª…Ã—2)')
        if stt_breakdown.get('standalone', 0) > 0:
            deps.append(f'ë…ë¦½ STT {stt_breakdown["standalone"]}ì±„ë„')
        
        if deps:
            return ' + '.join(deps)
        else:
            return 'ì´ STT ì±„ë„ ì²˜ë¦¬'
    
    def _format_nlp_dependency(self, nlp_breakdown: Dict[str, int]) -> str:
        """NLP ì„œë¹„ìŠ¤ ì¢…ì†ì„± ì •ë³´ë¥¼ í¬ë§·íŒ…"""
        deps = []
        if nlp_breakdown.get('callbot', 0) > 0:
            deps.append(f'ì½œë´‡ {int(nlp_breakdown["callbot"])}ì¿¼ë¦¬/ì¼')
        if nlp_breakdown.get('chatbot', 0) > 0:
            deps.append(f'ì±—ë´‡ {int(nlp_breakdown["chatbot"])}ì¿¼ë¦¬/ì¼')
        if nlp_breakdown.get('advisor', 0) > 0:
            deps.append(f'ì–´ë“œë°”ì´ì € {int(nlp_breakdown["advisor"])}ì¿¼ë¦¬/ì¼')
        if nlp_breakdown.get('ta', 0) > 0:
            deps.append(f'TA {int(nlp_breakdown["ta"])}ì¿¼ë¦¬/ì¼')
        
        if deps:
            return ' + '.join(deps)
        else:
            return 'NLP ìì—°ì–´ ì²˜ë¦¬'
    
    def _format_aicm_dependency(self, aicm_breakdown: Dict[str, int]) -> str:
        """AICM ì„œë¹„ìŠ¤ ì¢…ì†ì„± ì •ë³´ë¥¼ í¬ë§·íŒ…"""
        deps = []
        if aicm_breakdown.get('callbot', 0) > 0:
            deps.append(f'ì½œë´‡ {int(aicm_breakdown["callbot"])}ì¿¼ë¦¬/ì¼')
        if aicm_breakdown.get('chatbot', 0) > 0:
            deps.append(f'ì±—ë´‡ {int(aicm_breakdown["chatbot"])}ì¿¼ë¦¬/ì¼')
        if aicm_breakdown.get('advisor', 0) > 0:
            deps.append(f'ì–´ë“œë°”ì´ì € {int(aicm_breakdown["advisor"])}ì¿¼ë¦¬/ì¼')
        
        if deps:
            return ' + '.join(deps)
        else:
            return 'AICM ë²¡í„° ê²€ìƒ‰'
    
    def _format_infra_dependency(self, infra_breakdown: Dict, service_name: str) -> str:
        """ì¸í”„ë¼ ì„œë¹„ìŠ¤ ì¢…ì†ì„± ì •ë³´ë¥¼ í¬ë§·íŒ…"""
        total_channels = infra_breakdown.get('total_channels', 0)
        services = infra_breakdown.get('services', [])
        
        service_names = {
            'callbot': 'ì½œë´‡', 'chatbot': 'ì±—ë´‡', 'advisor': 'ì–´ë“œë°”ì´ì €',
            'ta': 'TA', 'qa': 'QA', 'stt': 'STT', 'tts': 'TTS'
        }
        
        service_list = [service_names.get(s, s) for s in services if s in service_names]
        
        if service_name == 'nginx':
            purpose = 'ë¡œë“œ ë°¸ëŸ°ì‹±'
        elif service_name == 'database':
            purpose = 'ë°ì´í„° ì €ì¥'
        elif service_name == 'vector_db':
            purpose = 'ë²¡í„° ê²€ìƒ‰ (ì–´ë“œë°”ì´ì € ì „ìš©)'
        elif service_name == 'gateway':
            purpose = 'API ë¼ìš°íŒ…'
        elif service_name == 'auth_service':
            purpose = 'ì¸ì¦ ê´€ë¦¬'
        else:
            purpose = 'ì¸í”„ë¼ ì§€ì›'
        
        if service_list:
            return f'{purpose} (ì „ì²´ {total_channels}ì±„ë„: {", ".join(service_list)})'
        else:
            return f'{purpose} (ì „ì²´ {total_channels}ì±„ë„)'
    
    def _find_optimal_instance_combination(self, required_cores: float, total_channels: int = 0) -> List[Dict[str, int]]:
        """í•„ìš”í•œ ì½”ì–´ ìˆ˜ì— ë§ëŠ” ìµœì ì˜ ì¸ìŠ¤í„´ìŠ¤ ì¡°í•© ì°¾ê¸° (ì„œë²„ ëŒ€ìˆ˜ ìµœì†Œí™” ìš°ì„ )"""
        
        logging.info(f"ğŸ” ì„œë²„ í†µí•© ë¡œì§ ì‹œì‘ - í•„ìš” ì½”ì–´: {required_cores:.1f}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ AWS ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë“¤ (64ì½”ì–´ê¹Œì§€ë§Œ, ì½”ì–´ ìˆ˜ ê¸°ì¤€, ë‚´ë¦¼ì°¨ìˆœ)
        instance_types = [64, 48, 36, 32, 24, 16, 8]
        
        # STT ì„œë²„ì˜ ê²½ìš° ì±„ë„ë‹¹ 10GBë¡œ ê³„ì‚°
        total_storage = max(1.0, total_channels * 0.01)  # ì±„ë„ë‹¹ 10GB, ìµœì†Œ 1TB
        
        # ì„œë²„ ëŒ€ìˆ˜ ìµœì†Œí™”ë¥¼ ìœ„í•´ ê°€ì¥ í° ì¸ìŠ¤í„´ìŠ¤ë¶€í„° ì‚¬ìš©
        # í•„ìš”í•œ ì„œë²„ ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ìµœì†Œ ëŒ€ìˆ˜ë¡œ êµ¬ì„±
        for max_cores in instance_types:
            server_count = math.ceil(required_cores / max_cores)
            total_provided_cores = server_count * max_cores
            limit = required_cores * 1.5
            
            logging.info(f"  ğŸ”¸ {max_cores}ì½”ì–´ Ã— {server_count}ëŒ€ = {total_provided_cores}ì½”ì–´ (ì œí•œ: {limit:.1f})")
            
            # í•„ìš” ì½”ì–´ì˜ 150% ì´ë‚´ë¡œ ì œí•œ (ì„œë²„ ëŒ€ìˆ˜ ìµœì†Œí™” ìš°ì„ , í° ì„œë²„ ì‚¬ìš© ê¶Œì¥)
            if total_provided_cores <= limit:
                logging.info(f"  âœ… ì„ íƒë¨: {max_cores}ì½”ì–´ Ã— {server_count}ëŒ€")
                return [{
                    'cpu_cores': max_cores,
                    'count': server_count,
                    'ram_gb': max_cores * 2,  # 1ì½”ì–´ë‹¹ 2GB RAM
                    'storage_ssd_tb': total_storage
                }]
            else:
                logging.info(f"  âŒ ì œì™¸ë¨: {total_provided_cores} > {limit:.1f}")
        
        # ìœ„ ì¡°ê±´ì— ë§ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ í˜¼í•© ë°©ì‹ ì‚¬ìš© (ê°œì„ ëœ ë²„ì „)
        remaining_cores = required_cores
        selected_instances = []
        
        for cores in instance_types:
            if remaining_cores <= 0:
                break
                
            if remaining_cores >= cores:
                count = int(remaining_cores // cores)
                if count > 0:
                    selected_instances.append({
                        'cpu_cores': cores,
                        'count': count,
                        'ram_gb': cores * 2,
                        'storage_ssd_tb': total_storage
                    })
                    remaining_cores -= (count * cores)
        
        # ë‚¨ì€ ì½”ì–´ê°€ 5ì½”ì–´ ì´ìƒì¼ ë•Œë§Œ ì¶”ê°€ ì„œë²„ ìƒì„± (ì†Œìˆ˜ì  ì˜¤ì°¨ ë°©ì§€)
        if remaining_cores >= 5:
            suitable_for_remaining = [x for x in instance_types if x >= remaining_cores]
            if suitable_for_remaining:
                best_fit_remaining = min(suitable_for_remaining)
                selected_instances.append({
                    'cpu_cores': best_fit_remaining,
                    'count': 1,
                    'ram_gb': best_fit_remaining * 2,
                    'storage_ssd_tb': total_storage
                })
        
        return selected_instances if selected_instances else [{
            'cpu_cores': 8,  # ìµœì†Œ ì¸ìŠ¤í„´ìŠ¤
            'count': 1,
            'ram_gb': 16,
            'storage_ssd_tb': total_storage
        }]
